{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NFZeNoWDVGaD",
        "outputId": "afceb7d8-3d14-4551-826f-fa8abf36b1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting certifi==2022.12.7\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer==2.1.1\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Collecting cmake==3.25.0\n",
            "  Downloading cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama==0.4.6\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting contourpy==1.1.0\n",
            "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m200.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.11.0\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (4.4.2)\n",
            "Collecting filelock==3.9.0\n",
            "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting fonttools==4.42.1\n",
            "  Downloading fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2023.4.0\n",
            "  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.0/154.0 kB\u001b[0m \u001b[31m156.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym==0.26.2\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m211.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym-notices==0.0.8 in /usr/local/lib/python3.10/dist-packages (0.0.8)\n",
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m279.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==3.4\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m231.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio==2.31.1\n",
            "  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m304.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio-ffmpeg==0.4.8\n",
            "  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Jinja2==3.1.2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m246.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (1.4.5)\n",
            "Collecting lit==15.0.7\n",
            "  Downloading lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m295.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lz4==4.3.2\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m331.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe==2.1.2\n",
            "  Downloading MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting matplotlib==3.7.2\n",
            "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: moviepy==1.0.3 in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Collecting mpmath==1.2.1\n",
            "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 kB\u001b[0m \u001b[31m305.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nes-py==8.2.1\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m226.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx==3.0\n",
            "  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m226.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python==4.8.0.76 in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Collecting packaging==23.1\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==9.3.0\n",
            "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proglog==0.1.10 in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Collecting pyglet==1.5.21\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing==3.0.9\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m222.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Collecting requests==2.28.1\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m237.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Collecting sympy==1.11.1\n",
            "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensordict==0.2.1\n",
            "  Downloading tensordict-0.2.1-cp310-cp310-manylinux1_x86_64.whl (986 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m986.5/986.5 kB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchrl==0.2.1\n",
            "  Downloading torchrl-0.2.1-cp310-cp310-manylinux1_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.66.1\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m255.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing_extensions==4.4.0\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting urllib3==1.26.13\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m258.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from tensordict==0.2.1) (2.2.1+cu121)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from tensordict==0.2.1)\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch->tensordict==0.2.1)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->tensordict==0.2.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym, lit, nes-py\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827624 sha256=7d8563014084423a8060253222900270484c7588ead293d94826ec7fe16bbfbc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vpw6bf9f/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89987 sha256=d4c68beeb1865e2bf2a110817b3faba6833f9dd48da23b087e82f73380d72e65\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vpw6bf9f/wheels/f2/e7/88/15af55b62ca17c1afe66dadbad8ce6ccbef4fe5a9d39ae7e9d\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535722 sha256=8cc3fe7f2bac075dea2e3dada0eb123bc48e582939a9a3d92bae6652d557905d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vpw6bf9f/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built gym lit nes-py\n",
            "Installing collected packages: pyglet, mpmath, lit, cmake, urllib3, typing_extensions, tqdm, sympy, pyparsing, Pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, lz4, imageio-ffmpeg, idna, fsspec, fonttools, filelock, cycler, colorama, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, Jinja2, imageio, gym, contourpy, nvidia-cusolver-cu12, nes-py, matplotlib, torch, gym-super-mario-bros, tensordict, torchrl\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.27.9\n",
            "    Uninstalling cmake-3.27.9:\n",
            "      Successfully uninstalled cmake-3.27.9\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.4\n",
            "    Uninstalling tqdm-4.66.4:\n",
            "      Successfully uninstalled tqdm-4.66.4\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.2\n",
            "    Uninstalling pyparsing-3.1.2:\n",
            "      Successfully uninstalled pyparsing-3.1.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: imageio-ffmpeg\n",
            "    Found existing installation: imageio-ffmpeg 0.4.9\n",
            "    Uninstalling imageio-ffmpeg-0.4.9:\n",
            "      Successfully uninstalled imageio-ffmpeg-0.4.9\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.51.0\n",
            "    Uninstalling fonttools-4.51.0:\n",
            "      Successfully uninstalled fonttools-4.51.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.14.0\n",
            "    Uninstalling filelock-3.14.0:\n",
            "      Successfully uninstalled filelock-3.14.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.2.2\n",
            "    Uninstalling certifi-2024.2.2:\n",
            "      Successfully uninstalled certifi-2024.2.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.31.6\n",
            "    Uninstalling imageio-2.31.6:\n",
            "      Successfully uninstalled imageio-2.31.6\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.2.1\n",
            "    Uninstalling contourpy-1.2.1:\n",
            "      Successfully uninstalled contourpy-1.2.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlalchemy 2.0.30 requires typing-extensions>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.4.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.1 which is incompatible.\n",
            "huggingface-hub 0.20.3 requires fsspec>=2023.5.0, but you have fsspec 2023.4.0 which is incompatible.\n",
            "kaggle 1.6.12 requires certifi>=2023.7.22, but you have certifi 2022.12.7 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "pydantic 2.7.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.4.0 which is incompatible.\n",
            "pydantic-core 2.18.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "yfinance 0.2.38 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Jinja2-3.1.2 MarkupSafe-2.1.2 Pillow-9.3.0 certifi-2022.12.7 charset-normalizer-2.1.1 cmake-3.25.0 colorama-0.4.6 contourpy-1.1.0 cycler-0.11.0 filelock-3.9.0 fonttools-4.42.1 fsspec-2023.4.0 gym-0.26.2 gym-super-mario-bros-7.4.0 idna-3.4 imageio-2.31.1 imageio-ffmpeg-0.4.8 lit-15.0.7 lz4-4.3.2 matplotlib-3.7.2 mpmath-1.2.1 nes-py-8.2.1 networkx-3.0 numpy-1.23.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 packaging-23.1 pyglet-1.5.21 pyparsing-3.0.9 requests-2.28.1 sympy-1.11.1 tensordict-0.2.1 torch-2.1.2 torchrl-0.2.1 tqdm-4.66.1 triton-2.1.0 typing_extensions-4.4.0 urllib3-1.26.13\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "be5c8af2e18749d9bbe505fbc1a829a0",
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "cycler",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --no-cache-dir certifi==2022.12.7 charset-normalizer==2.1.1 cloudpickle==2.2.1 cmake==3.25.0 colorama==0.4.6 contourpy==1.1.0 cycler==0.11.0 decorator==4.4.2 filelock==3.9.0 fonttools==4.42.1 fsspec==2023.4.0 gym==0.26.2 gym-notices==0.0.8 gym-super-mario-bros==7.4.0 idna==3.4 imageio==2.31.1 imageio-ffmpeg==0.4.8 Jinja2==3.1.2 kiwisolver==1.4.5 lit==15.0.7 lz4==4.3.2 MarkupSafe==2.1.2 matplotlib==3.7.2 moviepy==1.0.3 mpmath==1.2.1 nes-py==8.2.1 networkx==3.0 numpy==1.23.5 opencv-python==4.8.0.76 packaging==23.1 Pillow==9.3.0 proglog==0.1.10 pyglet==1.5.21 pyparsing==3.0.9 python-dateutil==2.8.2 requests==2.28.1 six==1.16.0 sympy==1.11.1 tensordict==0.2.1 torchrl==0.2.1 tqdm==4.66.1 typing_extensions==4.4.0 urllib3==1.26.13\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG0P9QXJXjr8",
        "outputId": "74d50af6-c0de-436c-9197-bd11fa1689b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Collecting torch==2.2.1 (from torchvision)\n",
            "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.9.0)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.2.1->torchvision)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.4.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->torchvision)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Collecting triton==2.2.0 (from torch==2.2.1->torchvision)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.2.1)\n",
            "Installing collected packages: typing-extensions, triton, nvidia-nccl-cu12, torch\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
            "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.2\n",
            "    Uninstalling torch-2.1.2:\n",
            "      Successfully uninstalled torch-2.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "huggingface-hub 0.20.3 requires fsspec>=2023.5.0, but you have fsspec 2023.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.19.3 torch-2.2.1 triton-2.2.0 typing-extensions-4.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YQ9ZA7hYrrB",
        "outputId": "9b4af0e7-103f-45e9-dcf9-ca36a479386c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PonYPRZqRN7E",
        "outputId": "941e28eb-4c62-439b-b57f-3d29b5edd286"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensordict/_pytree.py:93: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/torchrl/data/replay_buffers/samplers.py:23: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
            "  warnings.warn(EXTENSION_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
        "\n",
        "from gym import Wrapper\n",
        "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
        "\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import os\n",
        "from PIL import Image\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sQiofZILRdxb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AgentNN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions, freeze=False):\n",
        "        \"\"\"\n",
        "        Neural network model for the agent in a reinforcement learning environment.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of the input tensor (channels, height, width).\n",
        "            n_actions (int): Number of possible actions the agent can take.\n",
        "            freeze (bool): If True, freezes the parameters of the network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Calculate the output size of the convolutional layers\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        # Linear layers\n",
        "        self.network = nn.Sequential(\n",
        "            self.conv_layers,\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "        # Freeze the network parameters if specified\n",
        "        if freeze:\n",
        "            self._freeze()\n",
        "\n",
        "        # Move the model to GPU if available\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        \"\"\"\n",
        "        Calculate the output size of the convolutional layers.\n",
        "\n",
        "        Args:\n",
        "            shape (tuple): Shape of the input tensor (channels, height, width).\n",
        "\n",
        "        Returns:\n",
        "            int: Size of the output tensor after passing through the convolutional layers.\n",
        "        \"\"\"\n",
        "        o = self.conv_layers(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def _freeze(self):\n",
        "        \"\"\"Freeze the parameters of the network.\"\"\"\n",
        "        for p in self.network.parameters():\n",
        "            p.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k4eKV3eURgEU"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 input_dims,\n",
        "                 num_actions,\n",
        "                 lr=0.00025,\n",
        "                 gamma=0.9,\n",
        "                 epsilon=1.0,\n",
        "                 eps_decay=0.99999975,\n",
        "                 eps_min=0.1,\n",
        "                 replay_buffer_capacity=10_000,\n",
        "                 batch_size=32,\n",
        "                 sync_network_rate=10000):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            input_dims (tuple): Dimensions of the input observation.\n",
        "            num_actions (int): Number of possible actions.\n",
        "            lr (float): Learning rate for the optimizer.\n",
        "            gamma (float): Discount factor for future rewards.\n",
        "            epsilon (float): Initial value of exploration rate.\n",
        "            eps_decay (float): Decay rate for exploration rate.\n",
        "            eps_min (float): Minimum value of exploration rate.\n",
        "            replay_buffer_capacity (int): Capacity of the replay buffer.\n",
        "            batch_size (int): Batch size for training.\n",
        "            sync_network_rate (int): Frequency of synchronizing target network with online network.\n",
        "        \"\"\"\n",
        "        self.num_actions = num_actions\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_decay = eps_decay\n",
        "        self.eps_min = eps_min\n",
        "        self.batch_size = batch_size\n",
        "        self.sync_network_rate = sync_network_rate\n",
        "\n",
        "        # Networks\n",
        "        self.online_network = AgentNN(input_dims, num_actions)\n",
        "        self.target_network = AgentNN(input_dims, num_actions, freeze=True)\n",
        "\n",
        "        # Optimizer and loss\n",
        "        self.optimizer = torch.optim.Adam(self.online_network.parameters(), lr=self.lr)\n",
        "        self.loss = torch.nn.MSELoss()\n",
        "        # self.loss = torch.nn.SmoothL1Loss() # Try this loss function instead!\n",
        "\n",
        "        # Replay buffer\n",
        "        storage = LazyMemmapStorage(replay_buffer_capacity)\n",
        "        self.replay_buffer = TensorDictReplayBuffer(storage=storage)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        \"\"\"\n",
        "        Choose an action based on the current observation.\n",
        "\n",
        "        Args:\n",
        "            observation (list): Current observation.\n",
        "\n",
        "        Returns:\n",
        "            int: Chosen action.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        \n",
        "        observation = torch.tensor(np.array(observation), dtype=torch.float32) \\\n",
        "                        .unsqueeze(0) \\\n",
        "                        .to(self.online_network.device)\n",
        "        \n",
        "        return self.online_network(observation).argmax().item()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay the exploration rate epsilon.\"\"\"\n",
        "        self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min)\n",
        "\n",
        "    def store_in_memory(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Store the transition tuple in the replay buffer.\n",
        "\n",
        "        Args:\n",
        "            state: Current state.\n",
        "            action: Action taken.\n",
        "            reward: Reward received.\n",
        "            next_state: Next state.\n",
        "            done: Flag indicating if the episode is done.\n",
        "        \"\"\"\n",
        "        self.replay_buffer.add(TensorDict({\n",
        "                                            \"state\": torch.tensor(np.array(state), dtype=torch.float32),\n",
        "                                            \"action\": torch.tensor(action),\n",
        "                                            \"reward\": torch.tensor(reward),\n",
        "                                            \"next_state\": torch.tensor(np.array(next_state), dtype=torch.float32),\n",
        "                                            \"done\": torch.tensor(done)\n",
        "                                          }, batch_size=[]))\n",
        "\n",
        "    def sync_networks(self):\n",
        "        \"\"\"Synchronize the target network with the online network.\"\"\"\n",
        "        if self.learn_step_counter % self.sync_network_rate == 0 and self.learn_step_counter > 0:\n",
        "            self.target_network.load_state_dict(self.online_network.state_dict())\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save the model parameters to a file.\"\"\"\n",
        "        torch.save(self.online_network.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        \"\"\"Load the model parameters from a file.\"\"\"\n",
        "        self.online_network.load_state_dict(torch.load(path))\n",
        "        self.target_network.load_state_dict(torch.load(path))\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Perform one step of learning.\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.sync_networks()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        samples = self.replay_buffer.sample(self.batch_size).to(self.online_network.device)\n",
        "\n",
        "        keys = (\"state\", \"action\", \"reward\", \"next_state\", \"done\")\n",
        "\n",
        "        states, actions, rewards, next_states, dones = [samples[key] for key in keys]\n",
        "\n",
        "        predicted_q_values = self.online_network(states)\n",
        "        predicted_q_values = predicted_q_values[np.arange(self.batch_size), actions.squeeze()]\n",
        "\n",
        "        target_q_values = self.target_network(next_states).max(dim=1)[0]\n",
        "        target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
        "\n",
        "        loss = self.loss(predicted_q_values, target_q_values)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.learn_step_counter += 1\n",
        "        self.decay_epsilon()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t8ox-tIEWCU2"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"\n",
        "        A wrapper for skipping frames in an environment.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): The environment to wrap.\n",
        "            skip (int): Number of frames to skip before returning a new observation.\n",
        "        \"\"\"\n",
        "        super().__init__(env)\n",
        "        self.skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take a step in the environment.\n",
        "\n",
        "        Args:\n",
        "            action: Action to take.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing the next state, total reward, done flag, truncation flag, and additional info.\n",
        "        \"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for _ in range(self.skip):\n",
        "            next_state, reward, done, trunc, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return next_state, total_reward, done, trunc, info\n",
        "\n",
        "\n",
        "def apply_wrappers(env):\n",
        "    \"\"\"\n",
        "    Apply a series of wrappers to the environment.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): The environment to wrap.\n",
        "\n",
        "    Returns:\n",
        "        gym.Env: The wrapped environment.\n",
        "    \"\"\"\n",
        "    env = SkipFrame(env, skip=4)  # Num of frames to apply one action to\n",
        "    env = ResizeObservation(env, shape=84)  # Resize frame from 240x256 to 84x84\n",
        "    env = GrayScaleObservation(env)\n",
        "    env = FrameStack(env, num_stack=4, lz4_compress=True)  # May need to change lz4_compress to False if issues arise\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mnenhICeWD6z"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import time\n",
        "\n",
        "def get_current_date_time_string():\n",
        "    \"\"\"\n",
        "    Get the current date and time as a formatted string.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string representing the current date and time.\n",
        "    \"\"\"\n",
        "    return datetime.datetime.now().strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
        "\n",
        "class Timer():\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the timer.\"\"\"\n",
        "        self.times = []\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.t = time.time()\n",
        "\n",
        "    def print(self, msg=''):\n",
        "        \"\"\"\n",
        "        Print the time taken since the timer was started.\n",
        "\n",
        "        Args:\n",
        "            msg (str): Additional message to print along with the time taken.\n",
        "        \"\"\"\n",
        "        print(f\"Time taken: {msg}\", time.time() - self.t)\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Get the time taken since the timer was started.\n",
        "\n",
        "        Returns:\n",
        "            float: Time taken.\n",
        "        \"\"\"\n",
        "        return time.time() - self.t\n",
        "\n",
        "    def store(self):\n",
        "        \"\"\"Store the time taken since the timer was started.\"\"\"\n",
        "        self.times.append(time.time() - self.t)\n",
        "\n",
        "    def average(self):\n",
        "        \"\"\"\n",
        "        Calculate the average time taken.\n",
        "\n",
        "        Returns:\n",
        "            float: Average time taken.\n",
        "        \"\"\"\n",
        "        return sum(self.times) / len(self.times)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFuHF2cVYSBl",
        "outputId": "4abbb0e8-8a3e-4957-cbd2-945284dd2d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward: 858.0 Epsilon: 0.933428876898244 Size of replay buffer: 10000 Learn step counter: 275562\n",
            "Total reward: 858.0\n",
            "Episode: 957\n",
            "Total reward: 651.0 Epsilon: 0.9332041809168768 Size of replay buffer: 10000 Learn step counter: 276525\n",
            "Total reward: 651.0\n",
            "Episode: 958\n",
            "Total reward: 785.0 Epsilon: 0.9331362927736807 Size of replay buffer: 10000 Learn step counter: 276816\n",
            "Total reward: 785.0\n",
            "Episode: 959\n",
            "Total reward: 218.0 Epsilon: 0.9331253284852838 Size of replay buffer: 10000 Learn step counter: 276863\n",
            "Total reward: 218.0\n",
            "Episode: 960\n",
            "Total reward: 813.0 Epsilon: 0.9330901036645983 Size of replay buffer: 10000 Learn step counter: 277014\n",
            "Total reward: 813.0\n",
            "Episode: 961\n",
            "Total reward: 696.0 Epsilon: 0.933013826656944 Size of replay buffer: 10000 Learn step counter: 277341\n",
            "Total reward: 696.0\n",
            "Episode: 962\n",
            "Total reward: 974.0 Epsilon: 0.9329018717011671 Size of replay buffer: 10000 Learn step counter: 277821\n",
            "Total reward: 974.0\n",
            "Episode: 963\n",
            "Total reward: 635.0 Epsilon: 0.9328645563679412 Size of replay buffer: 10000 Learn step counter: 277981\n",
            "Total reward: 635.0\n",
            "Episode: 964\n",
            "Total reward: 1048.0 Epsilon: 0.9324886876473206 Size of replay buffer: 10000 Learn step counter: 279593\n",
            "Total reward: 1048.0\n",
            "Episode: 965\n",
            "Total reward: 611.0 Epsilon: 0.9324646763697554 Size of replay buffer: 10000 Learn step counter: 279696\n",
            "Total reward: 611.0\n",
            "Episode: 966\n",
            "Total reward: 670.0 Epsilon: 0.9323758633280054 Size of replay buffer: 10000 Learn step counter: 280077\n",
            "Total reward: 670.0\n",
            "Episode: 967\n",
            "Total reward: 929.0 Epsilon: 0.9321852119348668 Size of replay buffer: 10000 Learn step counter: 280895\n",
            "Total reward: 929.0\n",
            "Episode: 968\n",
            "Total reward: 229.0 Epsilon: 0.9321751909964472 Size of replay buffer: 10000 Learn step counter: 280938\n",
            "Total reward: 229.0\n",
            "Episode: 969\n",
            "Total reward: 232.0 Epsilon: 0.932165636248512 Size of replay buffer: 10000 Learn step counter: 280979\n",
            "Total reward: 232.0\n",
            "Episode: 970\n",
            "Total reward: 1168.0 Epsilon: 0.9319160822787053 Size of replay buffer: 10000 Learn step counter: 282050\n",
            "Total reward: 1168.0\n",
            "Episode: 971\n",
            "Total reward: 637.0 Epsilon: 0.9318806701359822 Size of replay buffer: 10000 Learn step counter: 282202\n",
            "Total reward: 637.0\n",
            "Episode: 972\n",
            "Total reward: 637.0 Epsilon: 0.9318454923002685 Size of replay buffer: 10000 Learn step counter: 282353\n",
            "Total reward: 637.0\n",
            "Episode: 973\n",
            "Total reward: 246.0 Epsilon: 0.9318382705247839 Size of replay buffer: 10000 Learn step counter: 282384\n",
            "Total reward: 246.0\n",
            "Episode: 974\n",
            "Total reward: 223.0 Epsilon: 0.9318282533159646 Size of replay buffer: 10000 Learn step counter: 282427\n",
            "Total reward: 223.0\n",
            "Episode: 975\n",
            "Total reward: 233.0 Epsilon: 0.9318198668983744 Size of replay buffer: 10000 Learn step counter: 282463\n",
            "Total reward: 233.0\n",
            "Episode: 976\n",
            "Total reward: 597.0 Epsilon: 0.9317940092526089 Size of replay buffer: 10000 Learn step counter: 282574\n",
            "Total reward: 597.0\n",
            "Episode: 977\n",
            "Total reward: 709.0 Epsilon: 0.9317343763368281 Size of replay buffer: 10000 Learn step counter: 282830\n",
            "Total reward: 709.0\n",
            "Episode: 978\n",
            "Total reward: 980.0 Epsilon: 0.9316288633802887 Size of replay buffer: 10000 Learn step counter: 283283\n",
            "Total reward: 980.0\n",
            "Episode: 979\n",
            "Total reward: 936.0 Epsilon: 0.9314723628580676 Size of replay buffer: 10000 Learn step counter: 283955\n",
            "Total reward: 936.0\n",
            "Episode: 980\n",
            "Total reward: 243.0 Epsilon: 0.9314642125095305 Size of replay buffer: 10000 Learn step counter: 283990\n",
            "Total reward: 243.0\n",
            "Episode: 981\n",
            "Total reward: 1192.0 Epsilon: 0.931265133286544 Size of replay buffer: 10000 Learn step counter: 284845\n",
            "Total reward: 1192.0\n",
            "Episode: 982\n",
            "Total reward: 559.0 Epsilon: 0.9312078622347826 Size of replay buffer: 10000 Learn step counter: 285091\n",
            "Total reward: 559.0\n",
            "Episode: 983\n",
            "Total reward: 633.0 Epsilon: 0.931167821152585 Size of replay buffer: 10000 Learn step counter: 285263\n",
            "Total reward: 633.0\n",
            "Episode: 984\n",
            "Total reward: 1231.0 Epsilon: 0.9310211737511886 Size of replay buffer: 10000 Learn step counter: 285893\n",
            "Total reward: 1231.0\n",
            "Episode: 985\n",
            "Total reward: 235.0 Epsilon: 0.9310127945972825 Size of replay buffer: 10000 Learn step counter: 285929\n",
            "Total reward: 235.0\n",
            "Episode: 986\n",
            "Total reward: 250.0 Epsilon: 0.9310062775297145 Size of replay buffer: 10000 Learn step counter: 285957\n",
            "Total reward: 250.0\n",
            "Episode: 987\n",
            "Total reward: 616.0 Epsilon: 0.930990217807933 Size of replay buffer: 10000 Learn step counter: 286026\n",
            "Total reward: 616.0\n",
            "Episode: 988\n",
            "Total reward: 247.0 Epsilon: 0.9309820716781473 Size of replay buffer: 10000 Learn step counter: 286061\n",
            "Total reward: 247.0\n",
            "Episode: 989\n",
            "Total reward: 970.0 Epsilon: 0.9308617500062814 Size of replay buffer: 10000 Learn step counter: 286578\n",
            "Total reward: 970.0\n",
            "Episode: 990\n",
            "Total reward: 1024.0 Epsilon: 0.9307947303646316 Size of replay buffer: 10000 Learn step counter: 286866\n",
            "Total reward: 1024.0\n",
            "Episode: 991\n",
            "Total reward: 247.0 Epsilon: 0.9307868186420578 Size of replay buffer: 10000 Learn step counter: 286900\n",
            "Total reward: 247.0\n",
            "Episode: 992\n",
            "Total reward: 638.0 Epsilon: 0.9307523801625749 Size of replay buffer: 10000 Learn step counter: 287048\n",
            "Total reward: 638.0\n",
            "Episode: 993\n",
            "Total reward: 598.0 Epsilon: 0.9307277155482214 Size of replay buffer: 10000 Learn step counter: 287154\n",
            "Total reward: 598.0\n",
            "Episode: 994\n",
            "Total reward: 227.0 Epsilon: 0.9307179429572914 Size of replay buffer: 10000 Learn step counter: 287196\n",
            "Total reward: 227.0\n",
            "Episode: 995\n",
            "Total reward: 250.0 Epsilon: 0.9307102645649746 Size of replay buffer: 10000 Learn step counter: 287229\n",
            "Total reward: 250.0\n",
            "Episode: 996\n",
            "Total reward: 245.0 Epsilon: 0.9307023535603577 Size of replay buffer: 10000 Learn step counter: 287263\n",
            "Total reward: 245.0\n",
            "Episode: 997\n",
            "Total reward: 779.0 Epsilon: 0.9306609382219395 Size of replay buffer: 10000 Learn step counter: 287441\n",
            "Total reward: 779.0\n",
            "Episode: 998\n",
            "Total reward: 236.0 Epsilon: 0.9306518643208915 Size of replay buffer: 10000 Learn step counter: 287480\n",
            "Total reward: 236.0\n",
            "Episode: 999\n",
            "Total reward: 994.0 Epsilon: 0.9305639218640546 Size of replay buffer: 10000 Learn step counter: 287858\n",
            "Total reward: 994.0\n",
            "Episode: 1000\n",
            "Total reward: 224.0 Epsilon: 0.9305546162701994 Size of replay buffer: 10000 Learn step counter: 287898\n",
            "Total reward: 224.0\n",
            "Episode: 1001\n",
            "Total reward: 246.0 Epsilon: 0.9305467065885875 Size of replay buffer: 10000 Learn step counter: 287932\n",
            "Total reward: 246.0\n",
            "Episode: 1002\n",
            "Total reward: 233.0 Epsilon: 0.9305383317048672 Size of replay buffer: 10000 Learn step counter: 287968\n",
            "Total reward: 233.0\n",
            "Episode: 1003\n",
            "Total reward: 247.0 Epsilon: 0.9305313526926774 Size of replay buffer: 10000 Learn step counter: 287998\n",
            "Total reward: 247.0\n",
            "Episode: 1004\n",
            "Total reward: 703.0 Epsilon: 0.9304787791498879 Size of replay buffer: 10000 Learn step counter: 288224\n",
            "Total reward: 703.0\n",
            "Episode: 1005\n",
            "Total reward: 772.0 Epsilon: 0.930436443323286 Size of replay buffer: 10000 Learn step counter: 288406\n",
            "Total reward: 772.0\n",
            "Episode: 1006\n",
            "Total reward: 975.0 Epsilon: 0.9303236347295758 Size of replay buffer: 10000 Learn step counter: 288891\n",
            "Total reward: 975.0\n",
            "Episode: 1007\n",
            "Total reward: 754.0 Epsilon: 0.930257118959336 Size of replay buffer: 10000 Learn step counter: 289177\n",
            "Total reward: 754.0\n",
            "Episode: 1008\n",
            "Total reward: 812.0 Epsilon: 0.9302206070793982 Size of replay buffer: 10000 Learn step counter: 289334\n",
            "Total reward: 812.0\n",
            "Episode: 1009\n",
            "Total reward: 964.0 Epsilon: 0.9300938731397702 Size of replay buffer: 10000 Learn step counter: 289879\n",
            "Total reward: 964.0\n",
            "Episode: 1010\n",
            "Total reward: 1266.0 Epsilon: 0.9299636690957329 Size of replay buffer: 10000 Learn step counter: 290439\n",
            "Total reward: 1266.0\n",
            "Episode: 1011\n",
            "Total reward: 228.0 Epsilon: 0.9299534395503554 Size of replay buffer: 10000 Learn step counter: 290483\n",
            "Total reward: 228.0\n",
            "Episode: 1012\n",
            "Total reward: 910.0 Epsilon: 0.9297693269740298 Size of replay buffer: 10000 Learn step counter: 291275\n",
            "Total reward: 910.0\n",
            "Episode: 1013\n",
            "Total reward: 612.0 Epsilon: 0.9297460830284973 Size of replay buffer: 10000 Learn step counter: 291375\n",
            "Total reward: 612.0\n",
            "Episode: 1014\n",
            "Total reward: 247.0 Epsilon: 0.9297388775233736 Size of replay buffer: 10000 Learn step counter: 291406\n",
            "Total reward: 247.0\n",
            "Episode: 1015\n",
            "Total reward: 616.0 Epsilon: 0.92971912077967 Size of replay buffer: 10000 Learn step counter: 291491\n",
            "Total reward: 616.0\n",
            "Episode: 1016\n",
            "Total reward: 244.0 Epsilon: 0.9297107533441894 Size of replay buffer: 10000 Learn step counter: 291527\n",
            "Total reward: 244.0\n",
            "Episode: 1017\n",
            "Total reward: 237.0 Epsilon: 0.9297023859840154 Size of replay buffer: 10000 Learn step counter: 291563\n",
            "Total reward: 237.0\n",
            "Episode: 1018\n",
            "Total reward: 611.0 Epsilon: 0.9296782140331917 Size of replay buffer: 10000 Learn step counter: 291667\n",
            "Total reward: 611.0\n",
            "Episode: 1019\n",
            "Total reward: 614.0 Epsilon: 0.9296565992632834 Size of replay buffer: 10000 Learn step counter: 291760\n",
            "Total reward: 614.0\n",
            "Episode: 1020\n",
            "Total reward: 1290.0 Epsilon: 0.9295545750374283 Size of replay buffer: 10000 Learn step counter: 292199\n",
            "Total reward: 1290.0\n",
            "Episode: 1021\n",
            "Total reward: 637.0 Epsilon: 0.9295197173900873 Size of replay buffer: 10000 Learn step counter: 292349\n",
            "Total reward: 637.0\n",
            "Episode: 1022\n",
            "Total reward: 233.0 Epsilon: 0.9295106546158898 Size of replay buffer: 10000 Learn step counter: 292388\n",
            "Total reward: 233.0\n",
            "Episode: 1023\n",
            "Total reward: 1046.0 Epsilon: 0.9294788194171663 Size of replay buffer: 10000 Learn step counter: 292525\n",
            "Total reward: 1046.0\n",
            "Episode: 1024\n",
            "Total reward: 1023.0 Epsilon: 0.9294109699313948 Size of replay buffer: 10000 Learn step counter: 292817\n",
            "Total reward: 1023.0\n",
            "Episode: 1025\n",
            "Total reward: 638.0 Epsilon: 0.9293763500132326 Size of replay buffer: 10000 Learn step counter: 292966\n",
            "Total reward: 638.0\n",
            "Episode: 1026\n",
            "Total reward: 638.0 Epsilon: 0.9293426607269507 Size of replay buffer: 10000 Learn step counter: 293111\n",
            "Total reward: 638.0\n",
            "Episode: 1027\n",
            "Total reward: 1632.0 Epsilon: 0.9292244093824771 Size of replay buffer: 10000 Learn step counter: 293620\n",
            "Total reward: 1632.0\n",
            "Episode: 1028\n",
            "Total reward: 1248.0 Epsilon: 0.9290794616626908 Size of replay buffer: 10000 Learn step counter: 294244\n",
            "Total reward: 1248.0\n",
            "Episode: 1029\n",
            "Total reward: 1340.0 Epsilon: 0.9289419680599309 Size of replay buffer: 10000 Learn step counter: 294836\n",
            "Total reward: 1340.0\n",
            "Episode: 1030\n",
            "Total reward: 714.0 Epsilon: 0.9288683523168126 Size of replay buffer: 10000 Learn step counter: 295153\n",
            "Total reward: 714.0\n",
            "Episode: 1031\n",
            "Total reward: 605.0 Epsilon: 0.9288277152102566 Size of replay buffer: 10000 Learn step counter: 295328\n",
            "Total reward: 605.0\n",
            "Episode: 1032\n",
            "Total reward: 704.0 Epsilon: 0.9287157981997036 Size of replay buffer: 10000 Learn step counter: 295810\n",
            "Total reward: 704.0\n",
            "Episode: 1033\n",
            "Total reward: 609.0 Epsilon: 0.9286916518998319 Size of replay buffer: 10000 Learn step counter: 295914\n",
            "Total reward: 609.0\n",
            "Episode: 1034\n",
            "Total reward: 621.0 Epsilon: 0.9286712209056754 Size of replay buffer: 10000 Learn step counter: 296002\n",
            "Total reward: 621.0\n",
            "Episode: 1035\n",
            "Total reward: 245.0 Epsilon: 0.9286628629012523 Size of replay buffer: 10000 Learn step counter: 296038\n",
            "Total reward: 245.0\n",
            "Episode: 1036\n",
            "Total reward: 588.0 Epsilon: 0.9286370928611429 Size of replay buffer: 10000 Learn step counter: 296149\n",
            "Total reward: 588.0\n",
            "Episode: 1037\n",
            "Total reward: 598.0 Epsilon: 0.928613877221113 Size of replay buffer: 10000 Learn step counter: 296249\n",
            "Total reward: 598.0\n",
            "Episode: 1038\n",
            "Total reward: 628.0 Epsilon: 0.92856767982411 Size of replay buffer: 10000 Learn step counter: 296448\n",
            "Total reward: 628.0\n",
            "Episode: 1039\n",
            "Total reward: 583.0 Epsilon: 0.9285412160190343 Size of replay buffer: 10000 Learn step counter: 296562\n",
            "Total reward: 583.0\n",
            "Episode: 1040\n",
            "Total reward: 1200.0 Epsilon: 0.9283309252236224 Size of replay buffer: 10000 Learn step counter: 297468\n",
            "Total reward: 1200.0\n",
            "Episode: 1041\n",
            "Total reward: 238.0 Epsilon: 0.9283230344433063 Size of replay buffer: 10000 Learn step counter: 297502\n",
            "Total reward: 238.0\n",
            "Episode: 1042\n",
            "Total reward: 636.0 Epsilon: 0.9282859022599305 Size of replay buffer: 10000 Learn step counter: 297662\n",
            "Total reward: 636.0\n",
            "Episode: 1043\n",
            "Total reward: 243.0 Epsilon: 0.9282777797928051 Size of replay buffer: 10000 Learn step counter: 297697\n",
            "Total reward: 243.0\n",
            "Episode: 1044\n",
            "Total reward: 246.0 Epsilon: 0.9282694253293367 Size of replay buffer: 10000 Learn step counter: 297733\n",
            "Total reward: 246.0\n",
            "Episode: 1045\n",
            "Total reward: 636.0 Epsilon: 0.9282329914648526 Size of replay buffer: 10000 Learn step counter: 297890\n",
            "Total reward: 636.0\n",
            "Episode: 1046\n",
            "Total reward: 234.0 Epsilon: 0.9282234771242606 Size of replay buffer: 10000 Learn step counter: 297931\n",
            "Total reward: 234.0\n",
            "Episode: 1047\n",
            "Total reward: 948.0 Epsilon: 0.9280649964763802 Size of replay buffer: 10000 Learn step counter: 298614\n",
            "Total reward: 948.0\n",
            "Episode: 1048\n",
            "Total reward: 623.0 Epsilon: 0.9280137222953703 Size of replay buffer: 10000 Learn step counter: 298835\n",
            "Total reward: 623.0\n",
            "Episode: 1049\n",
            "Total reward: 235.0 Epsilon: 0.9280049062057817 Size of replay buffer: 10000 Learn step counter: 298873\n",
            "Total reward: 235.0\n",
            "Episode: 1050\n",
            "Total reward: 636.0 Epsilon: 0.9279687147156493 Size of replay buffer: 10000 Learn step counter: 299029\n",
            "Total reward: 636.0\n",
            "Episode: 1051\n",
            "Total reward: 632.0 Epsilon: 0.9279478356518459 Size of replay buffer: 10000 Learn step counter: 299119\n",
            "Total reward: 632.0\n",
            "Episode: 1052\n",
            "Total reward: 231.0 Epsilon: 0.9279380922495052 Size of replay buffer: 10000 Learn step counter: 299161\n",
            "Total reward: 231.0\n",
            "Episode: 1053\n",
            "Total reward: 662.0 Epsilon: 0.9278054065729603 Size of replay buffer: 10000 Learn step counter: 299733\n",
            "Total reward: 662.0\n",
            "Episode: 1054\n",
            "Total reward: 598.0 Epsilon: 0.9277729339479408 Size of replay buffer: 10000 Learn step counter: 299873\n",
            "Total reward: 598.0\n",
            "Episode: 1055\n",
            "Total reward: 811.0 Epsilon: 0.9277351279664537 Size of replay buffer: 10000 Learn step counter: 300036\n",
            "Total reward: 811.0\n",
            "Episode: 1056\n",
            "Total reward: 722.0 Epsilon: 0.9276623036081881 Size of replay buffer: 10000 Learn step counter: 300350\n",
            "Total reward: 722.0\n",
            "Episode: 1057\n",
            "Total reward: 602.0 Epsilon: 0.9276379527892769 Size of replay buffer: 10000 Learn step counter: 300455\n",
            "Total reward: 602.0\n",
            "Episode: 1058\n",
            "Total reward: 784.0 Epsilon: 0.9275688463273939 Size of replay buffer: 10000 Learn step counter: 300753\n",
            "Total reward: 784.0\n",
            "Episode: 1059\n",
            "Total reward: 887.0 Epsilon: 0.9273536752889251 Size of replay buffer: 10000 Learn step counter: 301681\n",
            "Total reward: 887.0\n",
            "Episode: 1060\n",
            "Total reward: 230.0 Epsilon: 0.9273455609787512 Size of replay buffer: 10000 Learn step counter: 301716\n",
            "Total reward: 230.0\n",
            "Episode: 1061\n",
            "Total reward: 1102.0 Epsilon: 0.9270370380380776 Size of replay buffer: 10000 Learn step counter: 303047\n",
            "Total reward: 1102.0\n",
            "Episode: 1062\n",
            "Total reward: 246.0 Epsilon: 0.9270298535279738 Size of replay buffer: 10000 Learn step counter: 303078\n",
            "Total reward: 246.0\n",
            "Episode: 1063\n",
            "Total reward: 242.0 Epsilon: 0.9270210467850953 Size of replay buffer: 10000 Learn step counter: 303116\n",
            "Total reward: 242.0\n",
            "Episode: 1064\n",
            "Total reward: 241.0 Epsilon: 0.9270127036321745 Size of replay buffer: 10000 Learn step counter: 303152\n",
            "Total reward: 241.0\n",
            "Episode: 1065\n",
            "Total reward: 233.0 Epsilon: 0.9270034335503284 Size of replay buffer: 10000 Learn step counter: 303192\n",
            "Total reward: 233.0\n",
            "Episode: 1066\n",
            "Total reward: 246.0 Epsilon: 0.9269948588071534 Size of replay buffer: 10000 Learn step counter: 303229\n",
            "Total reward: 246.0\n",
            "Episode: 1067\n",
            "Total reward: 230.0 Epsilon: 0.9269865158899232 Size of replay buffer: 10000 Learn step counter: 303265\n",
            "Total reward: 230.0\n",
            "Episode: 1068\n",
            "Total reward: 613.0 Epsilon: 0.9269719159654456 Size of replay buffer: 10000 Learn step counter: 303328\n",
            "Total reward: 613.0\n",
            "Episode: 1069\n",
            "Total reward: 622.0 Epsilon: 0.9269471197952404 Size of replay buffer: 10000 Learn step counter: 303435\n",
            "Total reward: 622.0\n",
            "Episode: 1070\n",
            "Total reward: 232.0 Epsilon: 0.926937618634767 Size of replay buffer: 10000 Learn step counter: 303476\n",
            "Total reward: 232.0\n",
            "Episode: 1071\n",
            "Total reward: 628.0 Epsilon: 0.9269165310411764 Size of replay buffer: 10000 Learn step counter: 303567\n",
            "Total reward: 628.0\n",
            "Episode: 1072\n",
            "Total reward: 230.0 Epsilon: 0.9269079571018459 Size of replay buffer: 10000 Learn step counter: 303604\n",
            "Total reward: 230.0\n",
            "Episode: 1073\n",
            "Total reward: 1317.0 Epsilon: 0.9268481734591747 Size of replay buffer: 10000 Learn step counter: 303862\n",
            "Total reward: 1317.0\n",
            "Episode: 1074\n",
            "Total reward: 716.0 Epsilon: 0.9267995152012698 Size of replay buffer: 10000 Learn step counter: 304072\n",
            "Total reward: 716.0\n",
            "Episode: 1075\n",
            "Total reward: 1269.0 Epsilon: 0.9266978045232243 Size of replay buffer: 10000 Learn step counter: 304511\n",
            "Total reward: 1269.0\n",
            "Episode: 1076\n",
            "Total reward: 247.0 Epsilon: 0.926691317660485 Size of replay buffer: 10000 Learn step counter: 304539\n",
            "Total reward: 247.0\n",
            "Episode: 1077\n",
            "Total reward: 597.0 Epsilon: 0.9266651389972604 Size of replay buffer: 10000 Learn step counter: 304652\n",
            "Total reward: 597.0\n",
            "Episode: 1078\n",
            "Total reward: 603.0 Epsilon: 0.9266378027754505 Size of replay buffer: 10000 Learn step counter: 304770\n",
            "Total reward: 603.0\n",
            "Episode: 1079\n",
            "Total reward: 969.0 Epsilon: 0.9265194323424054 Size of replay buffer: 10000 Learn step counter: 305281\n",
            "Total reward: 969.0\n",
            "Episode: 1080\n",
            "Total reward: 764.0 Epsilon: 0.9264272482335866 Size of replay buffer: 10000 Learn step counter: 305679\n",
            "Total reward: 764.0\n",
            "Episode: 1081\n",
            "Total reward: 249.0 Epsilon: 0.926420300054411 Size of replay buffer: 10000 Learn step counter: 305709\n",
            "Total reward: 249.0\n",
            "Episode: 1082\n",
            "Total reward: 231.0 Epsilon: 0.926411962308187 Size of replay buffer: 10000 Learn step counter: 305745\n",
            "Total reward: 231.0\n",
            "Episode: 1083\n",
            "Total reward: 243.0 Epsilon: 0.926404782642402 Size of replay buffer: 10000 Learn step counter: 305776\n",
            "Total reward: 243.0\n",
            "Episode: 1084\n",
            "Total reward: 231.0 Epsilon: 0.9263943606459167 Size of replay buffer: 10000 Learn step counter: 305821\n",
            "Total reward: 231.0\n",
            "Episode: 1085\n",
            "Total reward: 608.0 Epsilon: 0.9263732854113069 Size of replay buffer: 10000 Learn step counter: 305912\n",
            "Total reward: 608.0\n",
            "Episode: 1086\n",
            "Total reward: 638.0 Epsilon: 0.9263390102295528 Size of replay buffer: 10000 Learn step counter: 306060\n",
            "Total reward: 638.0\n",
            "Episode: 1087\n",
            "Total reward: 795.0 Epsilon: 0.9262831999785032 Size of replay buffer: 10000 Learn step counter: 306301\n",
            "Total reward: 795.0\n",
            "Episode: 1088\n",
            "Total reward: 218.0 Epsilon: 0.9262727793498156 Size of replay buffer: 10000 Learn step counter: 306346\n",
            "Total reward: 218.0\n",
            "Episode: 1089\n",
            "Total reward: 772.0 Epsilon: 0.9262396656857205 Size of replay buffer: 10000 Learn step counter: 306489\n",
            "Total reward: 772.0\n",
            "Episode: 1090\n",
            "Total reward: 246.0 Epsilon: 0.9262313295651987 Size of replay buffer: 10000 Learn step counter: 306525\n",
            "Total reward: 246.0\n",
            "Episode: 1091\n",
            "Total reward: 614.0 Epsilon: 0.9262109526975447 Size of replay buffer: 10000 Learn step counter: 306613\n",
            "Total reward: 614.0\n",
            "Episode: 1092\n",
            "Total reward: 677.0 Epsilon: 0.9260919422218169 Size of replay buffer: 10000 Learn step counter: 307127\n",
            "Total reward: 677.0\n",
            "Episode: 1093\n",
            "Total reward: 250.0 Epsilon: 0.9260852280787345 Size of replay buffer: 10000 Learn step counter: 307156\n",
            "Total reward: 250.0\n",
            "Episode: 1094\n",
            "Total reward: 226.0 Epsilon: 0.9260759672715986 Size of replay buffer: 10000 Learn step counter: 307196\n",
            "Total reward: 226.0\n",
            "Episode: 1095\n",
            "Total reward: 812.0 Epsilon: 0.9260396194986636 Size of replay buffer: 10000 Learn step counter: 307353\n",
            "Total reward: 812.0\n",
            "Episode: 1096\n",
            "Total reward: 608.0 Epsilon: 0.9259856792551029 Size of replay buffer: 10000 Learn step counter: 307586\n",
            "Total reward: 608.0\n",
            "Episode: 1097\n",
            "Total reward: 748.0 Epsilon: 0.9259236402851583 Size of replay buffer: 10000 Learn step counter: 307854\n",
            "Total reward: 748.0\n",
            "Episode: 1098\n",
            "Total reward: 584.0 Epsilon: 0.9258970203598321 Size of replay buffer: 10000 Learn step counter: 307969\n",
            "Total reward: 584.0\n",
            "Episode: 1099\n",
            "Total reward: 251.0 Epsilon: 0.9258903076299281 Size of replay buffer: 10000 Learn step counter: 307998\n",
            "Total reward: 251.0\n",
            "Episode: 1100\n",
            "Total reward: 664.0 Epsilon: 0.9257690239287072 Size of replay buffer: 10000 Learn step counter: 308522\n",
            "Total reward: 664.0\n",
            "Episode: 1101\n",
            "Total reward: 751.0 Epsilon: 0.9256623351834051 Size of replay buffer: 10000 Learn step counter: 308983\n",
            "Total reward: 751.0\n",
            "Episode: 1102\n",
            "Total reward: 249.0 Epsilon: 0.9256556241549628 Size of replay buffer: 10000 Learn step counter: 309012\n",
            "Total reward: 249.0\n",
            "Episode: 1103\n",
            "Total reward: 250.0 Epsilon: 0.9256482189386638 Size of replay buffer: 10000 Learn step counter: 309044\n",
            "Total reward: 250.0\n",
            "Episode: 1104\n",
            "Total reward: 252.0 Epsilon: 0.9256412766021868 Size of replay buffer: 10000 Learn step counter: 309074\n",
            "Total reward: 252.0\n",
            "Episode: 1105\n",
            "Total reward: 1157.0 Epsilon: 0.925407118952091 Size of replay buffer: 10000 Learn step counter: 310086\n",
            "Total reward: 1157.0\n",
            "Episode: 1106\n",
            "Total reward: 247.0 Epsilon: 0.9253992530240258 Size of replay buffer: 10000 Learn step counter: 310120\n",
            "Total reward: 247.0\n",
            "Episode: 1107\n",
            "Total reward: 627.0 Epsilon: 0.9253816706030518 Size of replay buffer: 10000 Learn step counter: 310196\n",
            "Total reward: 627.0\n",
            "Episode: 1108\n",
            "Total reward: 247.0 Epsilon: 0.9253735735478454 Size of replay buffer: 10000 Learn step counter: 310231\n",
            "Total reward: 247.0\n",
            "Episode: 1109\n",
            "Total reward: 999.0 Epsilon: 0.9252673930092183 Size of replay buffer: 10000 Learn step counter: 310690\n",
            "Total reward: 999.0\n",
            "Episode: 1110\n",
            "Total reward: 246.0 Epsilon: 0.9252599908987564 Size of replay buffer: 10000 Learn step counter: 310722\n",
            "Total reward: 246.0\n",
            "Episode: 1111\n",
            "Total reward: 717.0 Epsilon: 0.9252213620956844 Size of replay buffer: 10000 Learn step counter: 310889\n",
            "Total reward: 717.0\n",
            "Episode: 1112\n",
            "Total reward: 246.0 Epsilon: 0.9252141916570163 Size of replay buffer: 10000 Learn step counter: 310920\n",
            "Total reward: 246.0\n",
            "Episode: 1113\n",
            "Total reward: 1209.0 Epsilon: 0.925024079651517 Size of replay buffer: 10000 Learn step counter: 311742\n",
            "Total reward: 1209.0\n",
            "Episode: 1114\n",
            "Total reward: 2037.0 Epsilon: 0.9245859532283457 Size of replay buffer: 10000 Learn step counter: 313637\n",
            "Total reward: 2037.0\n",
            "Episode: 1115\n",
            "Total reward: 630.0 Epsilon: 0.9245644568521391 Size of replay buffer: 10000 Learn step counter: 313730\n",
            "Total reward: 630.0\n",
            "Episode: 1116\n",
            "Total reward: 759.0 Epsilon: 0.9245274750088787 Size of replay buffer: 10000 Learn step counter: 313890\n",
            "Total reward: 759.0\n",
            "Episode: 1117\n",
            "Total reward: 635.0 Epsilon: 0.9245078290063159 Size of replay buffer: 10000 Learn step counter: 313975\n",
            "Total reward: 635.0\n",
            "Episode: 1118\n",
            "Total reward: 636.0 Epsilon: 0.9244720050175532 Size of replay buffer: 10000 Learn step counter: 314130\n",
            "Total reward: 636.0\n",
            "Episode: 1119\n",
            "Total reward: 239.0 Epsilon: 0.9244634536899868 Size of replay buffer: 10000 Learn step counter: 314167\n",
            "Total reward: 239.0\n",
            "Episode: 1120\n",
            "Total reward: 610.0 Epsilon: 0.9244246270354384 Size of replay buffer: 10000 Learn step counter: 314335\n",
            "Total reward: 610.0\n",
            "Episode: 1121\n",
            "Total reward: 231.0 Epsilon: 0.9244149206265984 Size of replay buffer: 10000 Learn step counter: 314377\n",
            "Total reward: 231.0\n",
            "Episode: 1122\n",
            "Total reward: 706.0 Epsilon: 0.9243157824303356 Size of replay buffer: 10000 Learn step counter: 314806\n",
            "Total reward: 706.0\n",
            "Episode: 1123\n",
            "Total reward: 653.0 Epsilon: 0.924299375968752 Size of replay buffer: 10000 Learn step counter: 314877\n",
            "Total reward: 653.0\n",
            "Episode: 1124\n",
            "Total reward: 615.0 Epsilon: 0.9242792726734326 Size of replay buffer: 10000 Learn step counter: 314964\n",
            "Total reward: 615.0\n",
            "Episode: 1125\n",
            "Total reward: 650.0 Epsilon: 0.9241729866553376 Size of replay buffer: 10000 Learn step counter: 315424\n",
            "Total reward: 650.0\n",
            "Episode: 1126\n",
            "Total reward: 787.0 Epsilon: 0.924107603721298 Size of replay buffer: 10000 Learn step counter: 315707\n",
            "Total reward: 787.0\n",
            "Episode: 1127\n",
            "Total reward: 637.0 Epsilon: 0.9240724882951596 Size of replay buffer: 10000 Learn step counter: 315859\n",
            "Total reward: 637.0\n",
            "Episode: 1128\n",
            "Total reward: 639.0 Epsilon: 0.9240394532900731 Size of replay buffer: 10000 Learn step counter: 316002\n",
            "Total reward: 639.0\n",
            "Episode: 1129\n",
            "Total reward: 232.0 Epsilon: 0.9240299819330324 Size of replay buffer: 10000 Learn step counter: 316043\n",
            "Total reward: 232.0\n",
            "Episode: 1130\n",
            "Total reward: 1019.0 Epsilon: 0.9239602202942019 Size of replay buffer: 10000 Learn step counter: 316345\n",
            "Total reward: 1019.0\n",
            "Episode: 1131\n",
            "Total reward: 700.0 Epsilon: 0.923908018009951 Size of replay buffer: 10000 Learn step counter: 316571\n",
            "Total reward: 700.0\n",
            "Episode: 1132\n",
            "Total reward: 609.0 Epsilon: 0.9238874612826721 Size of replay buffer: 10000 Learn step counter: 316660\n",
            "Total reward: 609.0\n",
            "Episode: 1133\n",
            "Total reward: 1212.0 Epsilon: 0.9237253332400795 Size of replay buffer: 10000 Learn step counter: 317362\n",
            "Total reward: 1212.0\n",
            "Episode: 1134\n",
            "Total reward: 582.0 Epsilon: 0.9237003929896578 Size of replay buffer: 10000 Learn step counter: 317470\n",
            "Total reward: 582.0\n",
            "Episode: 1135\n",
            "Total reward: 633.0 Epsilon: 0.9236611365522497 Size of replay buffer: 10000 Learn step counter: 317640\n",
            "Total reward: 633.0\n",
            "Episode: 1136\n",
            "Total reward: 215.0 Epsilon: 0.923650283596298 Size of replay buffer: 10000 Learn step counter: 317687\n",
            "Total reward: 215.0\n",
            "Episode: 1137\n",
            "Total reward: 811.0 Epsilon: 0.9236131074159069 Size of replay buffer: 10000 Learn step counter: 317848\n",
            "Total reward: 811.0\n",
            "Episode: 1138\n",
            "Total reward: 1271.0 Epsilon: 0.9234854267139606 Size of replay buffer: 10000 Learn step counter: 318401\n",
            "Total reward: 1271.0\n",
            "Episode: 1139\n",
            "Total reward: 636.0 Epsilon: 0.9234484880310484 Size of replay buffer: 10000 Learn step counter: 318561\n",
            "Total reward: 636.0\n",
            "Episode: 1140\n",
            "Total reward: 231.0 Epsilon: 0.9234390227313714 Size of replay buffer: 10000 Learn step counter: 318602\n",
            "Total reward: 231.0\n",
            "Episode: 1141\n",
            "Total reward: 610.0 Epsilon: 0.923403932710831 Size of replay buffer: 10000 Learn step counter: 318754\n",
            "Total reward: 610.0\n",
            "Episode: 1142\n",
            "Total reward: 586.0 Epsilon: 0.9233785394486628 Size of replay buffer: 10000 Learn step counter: 318864\n",
            "Total reward: 586.0\n",
            "Episode: 1143\n",
            "Total reward: 607.0 Epsilon: 0.9233584561813228 Size of replay buffer: 10000 Learn step counter: 318951\n",
            "Total reward: 607.0\n",
            "Episode: 1144\n",
            "Total reward: 606.0 Epsilon: 0.923331910003989 Size of replay buffer: 10000 Learn step counter: 319066\n",
            "Total reward: 606.0\n",
            "Episode: 1145\n",
            "Total reward: 232.0 Epsilon: 0.9233226767298999 Size of replay buffer: 10000 Learn step counter: 319106\n",
            "Total reward: 232.0\n",
            "Episode: 1146\n",
            "Total reward: 981.0 Epsilon: 0.9232181163445193 Size of replay buffer: 10000 Learn step counter: 319559\n",
            "Total reward: 981.0\n",
            "Episode: 1147\n",
            "Total reward: 246.0 Epsilon: 0.9232114230366015 Size of replay buffer: 10000 Learn step counter: 319588\n",
            "Total reward: 246.0\n",
            "Episode: 1148\n",
            "Total reward: 248.0 Epsilon: 0.9232033449709806 Size of replay buffer: 10000 Learn step counter: 319623\n",
            "Total reward: 248.0\n",
            "Episode: 1149\n",
            "Total reward: 241.0 Epsilon: 0.923195497774917 Size of replay buffer: 10000 Learn step counter: 319657\n",
            "Total reward: 241.0\n",
            "Episode: 1150\n",
            "Total reward: 646.0 Epsilon: 0.9230784901464469 Size of replay buffer: 10000 Learn step counter: 320164\n",
            "Total reward: 646.0\n",
            "Episode: 1151\n",
            "Total reward: 610.0 Epsilon: 0.923055413469765 Size of replay buffer: 10000 Learn step counter: 320264\n",
            "Total reward: 610.0\n",
            "Episode: 1152\n",
            "Total reward: 228.0 Epsilon: 0.9230461829606279 Size of replay buffer: 10000 Learn step counter: 320304\n",
            "Total reward: 228.0\n",
            "Episode: 1153\n",
            "Total reward: 1020.0 Epsilon: 0.9229880328755711 Size of replay buffer: 10000 Learn step counter: 320556\n",
            "Total reward: 1020.0\n",
            "Episode: 1154\n",
            "Total reward: 1021.0 Epsilon: 0.9229213493908209 Size of replay buffer: 10000 Learn step counter: 320845\n",
            "Total reward: 1021.0\n",
            "Episode: 1155\n",
            "Total reward: 245.0 Epsilon: 0.9229135045917097 Size of replay buffer: 10000 Learn step counter: 320879\n",
            "Total reward: 245.0\n",
            "Episode: 1156\n",
            "Total reward: 246.0 Epsilon: 0.9229061213122822 Size of replay buffer: 10000 Learn step counter: 320911\n",
            "Total reward: 246.0\n",
            "Episode: 1157\n",
            "Total reward: 1301.0 Epsilon: 0.9228200643181704 Size of replay buffer: 10000 Learn step counter: 321284\n",
            "Total reward: 1301.0\n",
            "Episode: 1158\n",
            "Total reward: 246.0 Epsilon: 0.9228124510830915 Size of replay buffer: 10000 Learn step counter: 321317\n",
            "Total reward: 246.0\n",
            "Episode: 1159\n",
            "Total reward: 608.0 Epsilon: 0.9227969941020543 Size of replay buffer: 10000 Learn step counter: 321384\n",
            "Total reward: 608.0\n",
            "Episode: 1160\n",
            "Total reward: 774.0 Epsilon: 0.9227167142457456 Size of replay buffer: 10000 Learn step counter: 321732\n",
            "Total reward: 774.0\n",
            "Episode: 1161\n",
            "Total reward: 234.0 Epsilon: 0.9227079484775007 Size of replay buffer: 10000 Learn step counter: 321770\n",
            "Total reward: 234.0\n",
            "Episode: 1162\n",
            "Total reward: 639.0 Epsilon: 0.9226747315851063 Size of replay buffer: 10000 Learn step counter: 321914\n",
            "Total reward: 639.0\n",
            "Episode: 1163\n",
            "Total reward: 238.0 Epsilon: 0.922666427548851 Size of replay buffer: 10000 Learn step counter: 321950\n",
            "Total reward: 238.0\n",
            "Episode: 1164\n",
            "Total reward: 228.0 Epsilon: 0.922656278272699 Size of replay buffer: 10000 Learn step counter: 321994\n",
            "Total reward: 228.0\n",
            "Episode: 1165\n",
            "Total reward: 245.0 Epsilon: 0.9226479744025229 Size of replay buffer: 10000 Learn step counter: 322030\n",
            "Total reward: 245.0\n",
            "Episode: 1166\n",
            "Total reward: 690.0 Epsilon: 0.922470381739068 Size of replay buffer: 10000 Learn step counter: 322800\n",
            "Total reward: 690.0\n",
            "Episode: 1167\n",
            "Total reward: 240.0 Epsilon: 0.9224618489264336 Size of replay buffer: 10000 Learn step counter: 322837\n",
            "Total reward: 240.0\n",
            "Episode: 1168\n",
            "Total reward: 649.0 Epsilon: 0.9224401713249838 Size of replay buffer: 10000 Learn step counter: 322931\n",
            "Total reward: 649.0\n",
            "Episode: 1169\n",
            "Total reward: 241.0 Epsilon: 0.9224316387917945 Size of replay buffer: 10000 Learn step counter: 322968\n",
            "Total reward: 241.0\n",
            "Episode: 1170\n",
            "Total reward: 944.0 Epsilon: 0.9222605435709601 Size of replay buffer: 10000 Learn step counter: 323710\n",
            "Total reward: 944.0\n",
            "Episode: 1171\n",
            "Total reward: 238.0 Epsilon: 0.9222522432623808 Size of replay buffer: 10000 Learn step counter: 323746\n",
            "Total reward: 238.0\n",
            "Episode: 1172\n",
            "Total reward: 246.0 Epsilon: 0.9222444041506483 Size of replay buffer: 10000 Learn step counter: 323780\n",
            "Total reward: 246.0\n",
            "Episode: 1173\n",
            "Total reward: 681.0 Epsilon: 0.922150109469422 Size of replay buffer: 10000 Learn step counter: 324189\n",
            "Total reward: 681.0\n",
            "Episode: 1174\n",
            "Total reward: 809.0 Epsilon: 0.9221099968070969 Size of replay buffer: 10000 Learn step counter: 324363\n",
            "Total reward: 809.0\n",
            "Episode: 1175\n",
            "Total reward: 234.0 Epsilon: 0.9221016978534327 Size of replay buffer: 10000 Learn step counter: 324399\n",
            "Total reward: 234.0\n",
            "Episode: 1176\n",
            "Total reward: 1157.0 Epsilon: 0.9218486156428259 Size of replay buffer: 10000 Learn step counter: 325497\n",
            "Total reward: 1157.0\n",
            "Episode: 1177\n",
            "Total reward: 603.0 Epsilon: 0.9218262610821492 Size of replay buffer: 10000 Learn step counter: 325594\n",
            "Total reward: 603.0\n",
            "Episode: 1178\n",
            "Total reward: 593.0 Epsilon: 0.9217774055788805 Size of replay buffer: 10000 Learn step counter: 325806\n",
            "Total reward: 593.0\n",
            "Episode: 1179\n",
            "Total reward: 220.0 Epsilon: 0.9217681878497599 Size of replay buffer: 10000 Learn step counter: 325846\n",
            "Total reward: 220.0\n",
            "Episode: 1180\n",
            "Total reward: 601.0 Epsilon: 0.9217347743543922 Size of replay buffer: 10000 Learn step counter: 325991\n",
            "Total reward: 601.0\n",
            "Episode: 1181\n",
            "Total reward: 1460.0 Epsilon: 0.9215644995631403 Size of replay buffer: 10000 Learn step counter: 326730\n",
            "Total reward: 1460.0\n",
            "Episode: 1182\n",
            "Total reward: 239.0 Epsilon: 0.9215552839630694 Size of replay buffer: 10000 Learn step counter: 326770\n",
            "Total reward: 239.0\n",
            "Episode: 1183\n",
            "Total reward: 947.0 Epsilon: 0.921412684298965 Size of replay buffer: 10000 Learn step counter: 327389\n",
            "Total reward: 947.0\n",
            "Episode: 1184\n",
            "Total reward: 575.0 Epsilon: 0.92138112645101 Size of replay buffer: 10000 Learn step counter: 327526\n",
            "Total reward: 575.0\n",
            "Episode: 1185\n",
            "Total reward: 957.0 Epsilon: 0.9212380931164799 Size of replay buffer: 10000 Learn step counter: 328147\n",
            "Total reward: 957.0\n",
            "Episode: 1186\n",
            "Total reward: 735.0 Epsilon: 0.9211614032278921 Size of replay buffer: 10000 Learn step counter: 328480\n",
            "Total reward: 735.0\n",
            "Episode: 1187\n",
            "Total reward: 238.0 Epsilon: 0.9211533430998684 Size of replay buffer: 10000 Learn step counter: 328515\n",
            "Total reward: 238.0\n",
            "Episode: 1188\n",
            "Total reward: 597.0 Epsilon: 0.92111741881552 Size of replay buffer: 10000 Learn step counter: 328671\n",
            "Total reward: 597.0\n",
            "Episode: 1189\n",
            "Total reward: 626.0 Epsilon: 0.9210948517123831 Size of replay buffer: 10000 Learn step counter: 328769\n",
            "Total reward: 626.0\n",
            "Episode: 1190\n",
            "Total reward: 762.0 Epsilon: 0.9210013653177503 Size of replay buffer: 10000 Learn step counter: 329175\n",
            "Total reward: 762.0\n",
            "Episode: 1191\n",
            "Total reward: 785.0 Epsilon: 0.9206691739562609 Size of replay buffer: 10000 Learn step counter: 330618\n",
            "Total reward: 785.0\n",
            "Episode: 1192\n",
            "Total reward: 592.0 Epsilon: 0.9206376415731021 Size of replay buffer: 10000 Learn step counter: 330755\n",
            "Total reward: 592.0\n",
            "Episode: 1193\n",
            "Total reward: 233.0 Epsilon: 0.9206288955559563 Size of replay buffer: 10000 Learn step counter: 330793\n",
            "Total reward: 233.0\n",
            "Episode: 1194\n",
            "Total reward: 616.0 Epsilon: 0.9206125545360422 Size of replay buffer: 10000 Learn step counter: 330864\n",
            "Total reward: 616.0\n",
            "Episode: 1195\n",
            "Total reward: 246.0 Epsilon: 0.9206049595128464 Size of replay buffer: 10000 Learn step counter: 330897\n",
            "Total reward: 246.0\n",
            "Episode: 1196\n",
            "Total reward: 232.0 Epsilon: 0.9205955233591905 Size of replay buffer: 10000 Learn step counter: 330938\n",
            "Total reward: 232.0\n",
            "Episode: 1197\n",
            "Total reward: 701.0 Epsilon: 0.9204850585105867 Size of replay buffer: 10000 Learn step counter: 331418\n",
            "Total reward: 701.0\n",
            "Episode: 1198\n",
            "Total reward: 206.0 Epsilon: 0.920473552517828 Size of replay buffer: 10000 Learn step counter: 331468\n",
            "Total reward: 206.0\n",
            "Episode: 1199\n",
            "Total reward: 1174.0 Epsilon: 0.9202551960437294 Size of replay buffer: 10000 Learn step counter: 332417\n",
            "Total reward: 1174.0\n",
            "Episode: 1200\n",
            "Total reward: 1289.0 Epsilon: 0.9201571940717834 Size of replay buffer: 10000 Learn step counter: 332843\n",
            "Total reward: 1289.0\n",
            "Episode: 1201\n",
            "Total reward: 1020.0 Epsilon: 0.9201006061373642 Size of replay buffer: 10000 Learn step counter: 333089\n",
            "Total reward: 1020.0\n",
            "Episode: 1202\n",
            "Total reward: 608.0 Epsilon: 0.920078984024475 Size of replay buffer: 10000 Learn step counter: 333183\n",
            "Total reward: 608.0\n",
            "Episode: 1203\n",
            "Total reward: 243.0 Epsilon: 0.9200711633853701 Size of replay buffer: 10000 Learn step counter: 333217\n",
            "Total reward: 243.0\n",
            "Episode: 1204\n",
            "Total reward: 1278.0 Epsilon: 0.9199667412213285 Size of replay buffer: 10000 Learn step counter: 333671\n",
            "Total reward: 1278.0\n",
            "Episode: 1205\n",
            "Total reward: 239.0 Epsilon: 0.9199589215362832 Size of replay buffer: 10000 Learn step counter: 333705\n",
            "Total reward: 239.0\n",
            "Episode: 1206\n",
            "Total reward: 1119.0 Epsilon: 0.9196641219092037 Size of replay buffer: 10000 Learn step counter: 334987\n",
            "Total reward: 1119.0\n",
            "Episode: 1207\n",
            "Total reward: 811.0 Epsilon: 0.9196264164484539 Size of replay buffer: 10000 Learn step counter: 335151\n",
            "Total reward: 811.0\n",
            "Episode: 1208\n",
            "Total reward: 673.0 Epsilon: 0.9195057233868463 Size of replay buffer: 10000 Learn step counter: 335676\n",
            "Total reward: 673.0\n",
            "Episode: 1209\n",
            "Total reward: 635.0 Epsilon: 0.9194684841544888 Size of replay buffer: 10000 Learn step counter: 335838\n",
            "Total reward: 635.0\n",
            "Episode: 1210\n",
            "Total reward: 867.0 Epsilon: 0.9192363476416585 Size of replay buffer: 10000 Learn step counter: 336848\n",
            "Total reward: 867.0\n",
            "Episode: 1211\n",
            "Total reward: 235.0 Epsilon: 0.9192276149367434 Size of replay buffer: 10000 Learn step counter: 336886\n",
            "Total reward: 235.0\n",
            "Episode: 1212\n",
            "Total reward: 224.0 Epsilon: 0.9192188823147888 Size of replay buffer: 10000 Learn step counter: 336924\n",
            "Total reward: 224.0\n",
            "Episode: 1213\n",
            "Total reward: 246.0 Epsilon: 0.9192101497757936 Size of replay buffer: 10000 Learn step counter: 336962\n",
            "Total reward: 246.0\n",
            "Episode: 1214\n",
            "Total reward: 620.0 Epsilon: 0.9191890081828324 Size of replay buffer: 10000 Learn step counter: 337054\n",
            "Total reward: 620.0\n",
            "Episode: 1215\n",
            "Total reward: 636.0 Epsilon: 0.9191531605060617 Size of replay buffer: 10000 Learn step counter: 337210\n",
            "Total reward: 636.0\n",
            "Episode: 1216\n",
            "Total reward: 1207.0 Epsilon: 0.9189854303391232 Size of replay buffer: 10000 Learn step counter: 337940\n",
            "Total reward: 1207.0\n",
            "Episode: 1217\n",
            "Total reward: 604.0 Epsilon: 0.918963834432561 Size of replay buffer: 10000 Learn step counter: 338034\n",
            "Total reward: 604.0\n",
            "Episode: 1218\n",
            "Total reward: 597.0 Epsilon: 0.9189364956617317 Size of replay buffer: 10000 Learn step counter: 338153\n",
            "Total reward: 597.0\n",
            "Episode: 1219\n",
            "Total reward: 443.0 Epsilon: 0.9184759941084978 Size of replay buffer: 10000 Learn step counter: 340158\n",
            "Total reward: 443.0\n",
            "Episode: 1220\n",
            "Total reward: 626.0 Epsilon: 0.9184286938068754 Size of replay buffer: 10000 Learn step counter: 340364\n",
            "Total reward: 626.0\n",
            "Episode: 1221\n",
            "Total reward: 241.0 Epsilon: 0.9184197391696441 Size of replay buffer: 10000 Learn step counter: 340403\n",
            "Total reward: 241.0\n",
            "Episode: 1222\n",
            "Total reward: 797.0 Epsilon: 0.9183657835881718 Size of replay buffer: 10000 Learn step counter: 340638\n",
            "Total reward: 797.0\n",
            "Episode: 1223\n",
            "Total reward: 233.0 Epsilon: 0.918356370385954 Size of replay buffer: 10000 Learn step counter: 340679\n",
            "Total reward: 233.0\n",
            "Episode: 1224\n",
            "Total reward: 625.0 Epsilon: 0.918334100511209 Size of replay buffer: 10000 Learn step counter: 340776\n",
            "Total reward: 625.0\n",
            "Episode: 1225\n",
            "Total reward: 1288.0 Epsilon: 0.9182466333428797 Size of replay buffer: 10000 Learn step counter: 341157\n",
            "Total reward: 1288.0\n",
            "Episode: 1226\n",
            "Total reward: 228.0 Epsilon: 0.918237910040207 Size of replay buffer: 10000 Learn step counter: 341195\n",
            "Total reward: 228.0\n",
            "Episode: 1227\n",
            "Total reward: 249.0 Epsilon: 0.9182307937230896 Size of replay buffer: 10000 Learn step counter: 341226\n",
            "Total reward: 249.0\n",
            "Episode: 1228\n",
            "Total reward: 773.0 Epsilon: 0.9181495338834774 Size of replay buffer: 10000 Learn step counter: 341580\n",
            "Total reward: 773.0\n",
            "Episode: 1229\n",
            "Total reward: 235.0 Epsilon: 0.9181408115032453 Size of replay buffer: 10000 Learn step counter: 341618\n",
            "Total reward: 235.0\n",
            "Episode: 1230\n",
            "Total reward: 1229.0 Epsilon: 0.9179728070865897 Size of replay buffer: 10000 Learn step counter: 342350\n",
            "Total reward: 1229.0\n",
            "Episode: 1231\n",
            "Total reward: 239.0 Epsilon: 0.9179643158763334 Size of replay buffer: 10000 Learn step counter: 342387\n",
            "Total reward: 239.0\n",
            "Episode: 1232\n",
            "Total reward: 793.0 Epsilon: 0.9179069448922734 Size of replay buffer: 10000 Learn step counter: 342637\n",
            "Total reward: 793.0\n",
            "Episode: 1233\n",
            "Total reward: 629.0 Epsilon: 0.9178888164068625 Size of replay buffer: 10000 Learn step counter: 342716\n",
            "Total reward: 629.0\n",
            "Episode: 1234\n",
            "Total reward: 1332.0 Epsilon: 0.9178364982288624 Size of replay buffer: 10000 Learn step counter: 342944\n",
            "Total reward: 1332.0\n",
            "Episode: 1235\n",
            "Total reward: 615.0 Epsilon: 0.9178195184085841 Size of replay buffer: 10000 Learn step counter: 343018\n",
            "Total reward: 615.0\n",
            "Episode: 1236\n",
            "Total reward: 231.0 Epsilon: 0.9178112580690563 Size of replay buffer: 10000 Learn step counter: 343054\n",
            "Total reward: 231.0\n",
            "Episode: 1237\n",
            "Total reward: 1061.0 Epsilon: 0.9174579687030565 Size of replay buffer: 10000 Learn step counter: 344594\n",
            "Total reward: 1061.0\n",
            "Episode: 1238\n",
            "Total reward: 247.0 Epsilon: 0.9174501703424899 Size of replay buffer: 10000 Learn step counter: 344628\n",
            "Total reward: 247.0\n",
            "Episode: 1239\n",
            "Total reward: 812.0 Epsilon: 0.9174134730650343 Size of replay buffer: 10000 Learn step counter: 344788\n",
            "Total reward: 812.0\n",
            "Episode: 1240\n",
            "Total reward: 630.0 Epsilon: 0.9173930608397909 Size of replay buffer: 10000 Learn step counter: 344877\n",
            "Total reward: 630.0\n",
            "Episode: 1241\n",
            "Total reward: 605.0 Epsilon: 0.9173710436677825 Size of replay buffer: 10000 Learn step counter: 344973\n",
            "Total reward: 605.0\n",
            "Episode: 1242\n",
            "Total reward: 712.0 Epsilon: 0.9173272402407808 Size of replay buffer: 10000 Learn step counter: 345164\n",
            "Total reward: 712.0\n",
            "Episode: 1243\n",
            "Total reward: 250.0 Epsilon: 0.9173201309813277 Size of replay buffer: 10000 Learn step counter: 345195\n",
            "Total reward: 250.0\n",
            "Episode: 1244\n",
            "Total reward: 611.0 Epsilon: 0.9172992621831201 Size of replay buffer: 10000 Learn step counter: 345286\n",
            "Total reward: 611.0\n",
            "Episode: 1245\n",
            "Total reward: 640.0 Epsilon: 0.917267386583612 Size of replay buffer: 10000 Learn step counter: 345425\n",
            "Total reward: 640.0\n",
            "Episode: 1246\n",
            "Total reward: 597.0 Epsilon: 0.9172423913847608 Size of replay buffer: 10000 Learn step counter: 345534\n",
            "Total reward: 597.0\n",
            "Episode: 1247\n",
            "Total reward: 634.0 Epsilon: 0.9172043266106046 Size of replay buffer: 10000 Learn step counter: 345700\n",
            "Total reward: 634.0\n",
            "Episode: 1248\n",
            "Total reward: 609.0 Epsilon: 0.9171703906740913 Size of replay buffer: 10000 Learn step counter: 345848\n",
            "Total reward: 609.0\n",
            "Episode: 1249\n",
            "Total reward: 671.0 Epsilon: 0.916972532482263 Size of replay buffer: 10000 Learn step counter: 346711\n",
            "Total reward: 671.0\n",
            "Episode: 1250\n",
            "Total reward: 636.0 Epsilon: 0.9169544224513171 Size of replay buffer: 10000 Learn step counter: 346790\n",
            "Total reward: 636.0\n",
            "Episode: 1251\n",
            "Total reward: 794.0 Epsilon: 0.9166257531672273 Size of replay buffer: 10000 Learn step counter: 348224\n",
            "Total reward: 794.0\n",
            "Episode: 1252\n",
            "Total reward: 1785.0 Epsilon: 0.9164923938055797 Size of replay buffer: 10000 Learn step counter: 348806\n",
            "Total reward: 1785.0\n",
            "Episode: 1253\n",
            "Total reward: 247.0 Epsilon: 0.9164846036523656 Size of replay buffer: 10000 Learn step counter: 348840\n",
            "Total reward: 247.0\n",
            "Episode: 1254\n",
            "Total reward: 726.0 Epsilon: 0.9163496612129725 Size of replay buffer: 10000 Learn step counter: 349429\n",
            "Total reward: 726.0\n",
            "Episode: 1255\n",
            "Total reward: 733.0 Epsilon: 0.9162236717807614 Size of replay buffer: 10000 Learn step counter: 349979\n",
            "Total reward: 733.0\n",
            "Episode: 1256\n",
            "Total reward: 637.0 Epsilon: 0.9161888559383818 Size of replay buffer: 10000 Learn step counter: 350131\n",
            "Total reward: 637.0\n",
            "Episode: 1257\n",
            "Total reward: 626.0 Epsilon: 0.9161677838343882 Size of replay buffer: 10000 Learn step counter: 350223\n",
            "Total reward: 626.0\n",
            "Episode: 1258\n",
            "Total reward: 636.0 Epsilon: 0.9161311378513735 Size of replay buffer: 10000 Learn step counter: 350383\n",
            "Total reward: 636.0\n",
            "Episode: 1259\n",
            "Total reward: 766.0 Epsilon: 0.9160924321236198 Size of replay buffer: 10000 Learn step counter: 350552\n",
            "Total reward: 766.0\n",
            "Episode: 1260\n",
            "Total reward: 244.0 Epsilon: 0.9160844163489047 Size of replay buffer: 10000 Learn step counter: 350587\n",
            "Total reward: 244.0\n",
            "Episode: 1261\n",
            "Total reward: 638.0 Epsilon: 0.9160505218483105 Size of replay buffer: 10000 Learn step counter: 350735\n",
            "Total reward: 638.0\n",
            "Episode: 1262\n",
            "Total reward: 240.0 Epsilon: 0.9160420484191126 Size of replay buffer: 10000 Learn step counter: 350772\n",
            "Total reward: 240.0\n",
            "Episode: 1263\n",
            "Total reward: 1285.0 Epsilon: 0.9158327566973483 Size of replay buffer: 10000 Learn step counter: 351686\n",
            "Total reward: 1285.0\n",
            "Episode: 1264\n",
            "Total reward: 638.0 Epsilon: 0.9157991004577651 Size of replay buffer: 10000 Learn step counter: 351833\n",
            "Total reward: 638.0\n",
            "Episode: 1265\n",
            "Total reward: 1116.0 Epsilon: 0.9157219476240056 Size of replay buffer: 10000 Learn step counter: 352170\n",
            "Total reward: 1116.0\n",
            "Episode: 1266\n",
            "Total reward: 1058.0 Epsilon: 0.9156805121381755 Size of replay buffer: 10000 Learn step counter: 352351\n",
            "Total reward: 1058.0\n",
            "Episode: 1267\n",
            "Total reward: 592.0 Epsilon: 0.9156489217014916 Size of replay buffer: 10000 Learn step counter: 352489\n",
            "Total reward: 592.0\n",
            "Episode: 1268\n",
            "Total reward: 882.0 Epsilon: 0.9154303365887178 Size of replay buffer: 10000 Learn step counter: 353444\n",
            "Total reward: 882.0\n",
            "Episode: 1269\n",
            "Total reward: 247.0 Epsilon: 0.9154234708860803 Size of replay buffer: 10000 Learn step counter: 353474\n",
            "Total reward: 247.0\n",
            "Episode: 1270\n",
            "Total reward: 593.0 Epsilon: 0.915398754782943 Size of replay buffer: 10000 Learn step counter: 353582\n",
            "Total reward: 593.0\n",
            "Episode: 1271\n",
            "Total reward: 729.0 Epsilon: 0.9153591646379873 Size of replay buffer: 10000 Learn step counter: 353755\n",
            "Total reward: 729.0\n",
            "Episode: 1272\n",
            "Total reward: 637.0 Epsilon: 0.9153248393086221 Size of replay buffer: 10000 Learn step counter: 353905\n",
            "Total reward: 637.0\n",
            "Episode: 1273\n",
            "Total reward: 700.0 Epsilon: 0.9152234726832643 Size of replay buffer: 10000 Learn step counter: 354348\n",
            "Total reward: 700.0\n",
            "Episode: 1274\n",
            "Total reward: 580.0 Epsilon: 0.9151946435943276 Size of replay buffer: 10000 Learn step counter: 354474\n",
            "Total reward: 580.0\n",
            "Episode: 1275\n",
            "Total reward: 699.0 Epsilon: 0.9151397335561577 Size of replay buffer: 10000 Learn step counter: 354714\n",
            "Total reward: 699.0\n",
            "Episode: 1276\n",
            "Total reward: 606.0 Epsilon: 0.9151136524421453 Size of replay buffer: 10000 Learn step counter: 354828\n",
            "Total reward: 606.0\n",
            "Episode: 1277\n",
            "Total reward: 242.0 Epsilon: 0.9151047301264138 Size of replay buffer: 10000 Learn step counter: 354867\n",
            "Total reward: 242.0\n",
            "Episode: 1278\n",
            "Total reward: 632.0 Epsilon: 0.9150637801008801 Size of replay buffer: 10000 Learn step counter: 355046\n",
            "Total reward: 632.0\n",
            "Episode: 1279\n",
            "Total reward: 612.0 Epsilon: 0.9150413613100935 Size of replay buffer: 10000 Learn step counter: 355144\n",
            "Total reward: 612.0\n",
            "Episode: 1280\n",
            "Total reward: 587.0 Epsilon: 0.9150177992954536 Size of replay buffer: 10000 Learn step counter: 355247\n",
            "Total reward: 587.0\n",
            "Episode: 1281\n",
            "Total reward: 971.0 Epsilon: 0.9149027430271085 Size of replay buffer: 10000 Learn step counter: 355750\n",
            "Total reward: 971.0\n",
            "Episode: 1282\n",
            "Total reward: 618.0 Epsilon: 0.9148851313166144 Size of replay buffer: 10000 Learn step counter: 355827\n",
            "Total reward: 618.0\n",
            "Episode: 1283\n",
            "Total reward: 767.0 Epsilon: 0.9148391594880653 Size of replay buffer: 10000 Learn step counter: 356028\n",
            "Total reward: 767.0\n",
            "Episode: 1284\n",
            "Total reward: 957.0 Epsilon: 0.9147042306464024 Size of replay buffer: 10000 Learn step counter: 356618\n",
            "Total reward: 957.0\n",
            "Episode: 1285\n",
            "Total reward: 643.0 Epsilon: 0.9145242802697018 Size of replay buffer: 10000 Learn step counter: 357405\n",
            "Total reward: 643.0\n",
            "Episode: 1286\n",
            "Total reward: 591.0 Epsilon: 0.9145023319476097 Size of replay buffer: 10000 Learn step counter: 357501\n",
            "Total reward: 591.0\n",
            "Episode: 1287\n",
            "Total reward: 997.0 Epsilon: 0.9144140867194345 Size of replay buffer: 10000 Learn step counter: 357887\n",
            "Total reward: 997.0\n",
            "Episode: 1288\n",
            "Total reward: 234.0 Epsilon: 0.9144053998257864 Size of replay buffer: 10000 Learn step counter: 357925\n",
            "Total reward: 234.0\n",
            "Episode: 1289\n",
            "Total reward: 723.0 Epsilon: 0.914337050568182 Size of replay buffer: 10000 Learn step counter: 358224\n",
            "Total reward: 723.0\n",
            "Episode: 1290\n",
            "Total reward: 636.0 Epsilon: 0.9143013921140921 Size of replay buffer: 10000 Learn step counter: 358380\n",
            "Total reward: 636.0\n",
            "Episode: 1291\n",
            "Total reward: 990.0 Epsilon: 0.9141973962326035 Size of replay buffer: 10000 Learn step counter: 358835\n",
            "Total reward: 990.0\n",
            "Episode: 1292\n",
            "Total reward: 755.0 Epsilon: 0.9140966115129165 Size of replay buffer: 10000 Learn step counter: 359276\n",
            "Total reward: 755.0\n",
            "Episode: 1293\n",
            "Total reward: 235.0 Epsilon: 0.9140886132015575 Size of replay buffer: 10000 Learn step counter: 359311\n",
            "Total reward: 235.0\n",
            "Episode: 1294\n",
            "Total reward: 235.0 Epsilon: 0.9140801579199331 Size of replay buffer: 10000 Learn step counter: 359348\n",
            "Total reward: 235.0\n",
            "Episode: 1295\n",
            "Total reward: 241.0 Epsilon: 0.9140723882706395 Size of replay buffer: 10000 Learn step counter: 359382\n",
            "Total reward: 241.0\n",
            "Episode: 1296\n",
            "Total reward: 242.0 Epsilon: 0.9140648472035995 Size of replay buffer: 10000 Learn step counter: 359415\n",
            "Total reward: 242.0\n",
            "Episode: 1297\n",
            "Total reward: 589.0 Epsilon: 0.9140362831198661 Size of replay buffer: 10000 Learn step counter: 359540\n",
            "Total reward: 589.0\n",
            "Episode: 1298\n",
            "Total reward: 636.0 Epsilon: 0.9140006363954793 Size of replay buffer: 10000 Learn step counter: 359696\n",
            "Total reward: 636.0\n",
            "Episode: 1299\n",
            "Total reward: 751.0 Epsilon: 0.9139622491700845 Size of replay buffer: 10000 Learn step counter: 359864\n",
            "Total reward: 751.0\n",
            "Episode: 1300\n",
            "Total reward: 712.0 Epsilon: 0.9138893635779446 Size of replay buffer: 10000 Learn step counter: 360183\n",
            "Total reward: 712.0\n",
            "Episode: 1301\n",
            "Total reward: 735.0 Epsilon: 0.9138608049779894 Size of replay buffer: 10000 Learn step counter: 360308\n",
            "Total reward: 735.0\n",
            "Episode: 1302\n",
            "Total reward: 1309.0 Epsilon: 0.9138091732946558 Size of replay buffer: 10000 Learn step counter: 360534\n",
            "Total reward: 1309.0\n",
            "Episode: 1303\n",
            "Total reward: 238.0 Epsilon: 0.9138002636975358 Size of replay buffer: 10000 Learn step counter: 360573\n",
            "Total reward: 238.0\n",
            "Episode: 1304\n",
            "Total reward: 252.0 Epsilon: 0.9137936386688107 Size of replay buffer: 10000 Learn step counter: 360602\n",
            "Total reward: 252.0\n",
            "Episode: 1305\n",
            "Total reward: 246.0 Epsilon: 0.9137863283480279 Size of replay buffer: 10000 Learn step counter: 360634\n",
            "Total reward: 246.0\n",
            "Episode: 1306\n",
            "Total reward: 231.0 Epsilon: 0.9137771905292901 Size of replay buffer: 10000 Learn step counter: 360674\n",
            "Total reward: 231.0\n",
            "Episode: 1307\n",
            "Total reward: 636.0 Epsilon: 0.9137573160890443 Size of replay buffer: 10000 Learn step counter: 360761\n",
            "Total reward: 636.0\n",
            "Episode: 1308\n",
            "Total reward: 610.0 Epsilon: 0.9137214518137409 Size of replay buffer: 10000 Learn step counter: 360918\n",
            "Total reward: 610.0\n",
            "Episode: 1309\n",
            "Total reward: 242.0 Epsilon: 0.9137132283566511 Size of replay buffer: 10000 Learn step counter: 360954\n",
            "Total reward: 242.0\n",
            "Episode: 1310\n",
            "Total reward: 238.0 Epsilon: 0.9137056902526686 Size of replay buffer: 10000 Learn step counter: 360987\n",
            "Total reward: 238.0\n",
            "Episode: 1311\n",
            "Total reward: 1684.0 Epsilon: 0.9133925712519841 Size of replay buffer: 10000 Learn step counter: 362358\n",
            "Total reward: 1684.0\n",
            "Episode: 1312\n",
            "Total reward: 584.0 Epsilon: 0.9133669966148352 Size of replay buffer: 10000 Learn step counter: 362470\n",
            "Total reward: 584.0\n",
            "Episode: 1313\n",
            "Total reward: 243.0 Epsilon: 0.9133596897071755 Size of replay buffer: 10000 Learn step counter: 362502\n",
            "Total reward: 243.0\n",
            "Episode: 1314\n",
            "Total reward: 730.0 Epsilon: 0.9133263526827339 Size of replay buffer: 10000 Learn step counter: 362648\n",
            "Total reward: 730.0\n",
            "Episode: 1315\n",
            "Total reward: 243.0 Epsilon: 0.9133188177704629 Size of replay buffer: 10000 Learn step counter: 362681\n",
            "Total reward: 243.0\n",
            "Episode: 1316\n",
            "Total reward: 767.0 Epsilon: 0.9132793175808491 Size of replay buffer: 10000 Learn step counter: 362854\n",
            "Total reward: 767.0\n",
            "Episode: 1317\n",
            "Total reward: 1227.0 Epsilon: 0.9131277257776286 Size of replay buffer: 10000 Learn step counter: 363518\n",
            "Total reward: 1227.0\n",
            "Episode: 1318\n",
            "Total reward: 620.0 Epsilon: 0.913105810972446 Size of replay buffer: 10000 Learn step counter: 363614\n",
            "Total reward: 620.0\n",
            "Episode: 1319\n",
            "Total reward: 247.0 Epsilon: 0.9130989627036877 Size of replay buffer: 10000 Learn step counter: 363644\n",
            "Total reward: 247.0\n",
            "Episode: 1320\n",
            "Total reward: 237.0 Epsilon: 0.9130907448489753 Size of replay buffer: 10000 Learn step counter: 363680\n",
            "Total reward: 237.0\n",
            "Episode: 1321\n",
            "Total reward: 990.0 Epsilon: 0.912995788337455 Size of replay buffer: 10000 Learn step counter: 364096\n",
            "Total reward: 990.0\n",
            "Episode: 1322\n",
            "Total reward: 238.0 Epsilon: 0.9129871149175793 Size of replay buffer: 10000 Learn step counter: 364134\n",
            "Total reward: 238.0\n",
            "Episode: 1323\n",
            "Total reward: 249.0 Epsilon: 0.9129795828040085 Size of replay buffer: 10000 Learn step counter: 364167\n",
            "Total reward: 249.0\n",
            "Episode: 1324\n",
            "Total reward: 231.0 Epsilon: 0.9129713660237105 Size of replay buffer: 10000 Learn step counter: 364203\n",
            "Total reward: 231.0\n",
            "Episode: 1325\n",
            "Total reward: 823.0 Epsilon: 0.9126783492102635 Size of replay buffer: 10000 Learn step counter: 365487\n",
            "Total reward: 823.0\n",
            "Episode: 1326\n",
            "Total reward: 1326.0 Epsilon: 0.9126281532751762 Size of replay buffer: 10000 Learn step counter: 365707\n",
            "Total reward: 1326.0\n",
            "Episode: 1327\n",
            "Total reward: 251.0 Epsilon: 0.912621536744222 Size of replay buffer: 10000 Learn step counter: 365736\n",
            "Total reward: 251.0\n",
            "Episode: 1328\n",
            "Total reward: 239.0 Epsilon: 0.9126133231863245 Size of replay buffer: 10000 Learn step counter: 365772\n",
            "Total reward: 239.0\n",
            "Episode: 1329\n",
            "Total reward: 709.0 Epsilon: 0.9124593326620819 Size of replay buffer: 10000 Learn step counter: 366447\n",
            "Total reward: 709.0\n",
            "Episode: 1330\n",
            "Total reward: 231.0 Epsilon: 0.9124508924512341 Size of replay buffer: 10000 Learn step counter: 366484\n",
            "Total reward: 231.0\n",
            "Episode: 1331\n",
            "Total reward: 1515.0 Epsilon: 0.9121797066964964 Size of replay buffer: 10000 Learn step counter: 367673\n",
            "Total reward: 1515.0\n",
            "Episode: 1332\n",
            "Total reward: 844.0 Epsilon: 0.9119113372700067 Size of replay buffer: 10000 Learn step counter: 368850\n",
            "Total reward: 844.0\n",
            "Episode: 1333\n",
            "Total reward: 246.0 Epsilon: 0.9119031301038766 Size of replay buffer: 10000 Learn step counter: 368886\n",
            "Total reward: 246.0\n",
            "Episode: 1334\n",
            "Total reward: 1335.0 Epsilon: 0.9118545725489791 Size of replay buffer: 10000 Learn step counter: 369099\n",
            "Total reward: 1335.0\n",
            "Episode: 1335\n",
            "Total reward: 922.0 Epsilon: 0.9116842995859805 Size of replay buffer: 10000 Learn step counter: 369846\n",
            "Total reward: 922.0\n",
            "Episode: 1336\n",
            "Total reward: 1007.0 Epsilon: 0.9116111398473575 Size of replay buffer: 10000 Learn step counter: 370167\n",
            "Total reward: 1007.0\n",
            "Episode: 1337\n",
            "Total reward: 688.0 Epsilon: 0.9115441388825098 Size of replay buffer: 10000 Learn step counter: 370461\n",
            "Total reward: 688.0\n",
            "Episode: 1338\n",
            "Total reward: 237.0 Epsilon: 0.9115352513693703 Size of replay buffer: 10000 Learn step counter: 370500\n",
            "Total reward: 237.0\n",
            "Episode: 1339\n",
            "Total reward: 229.0 Epsilon: 0.9115272754698173 Size of replay buffer: 10000 Learn step counter: 370535\n",
            "Total reward: 229.0\n",
            "Episode: 1340\n",
            "Total reward: 735.0 Epsilon: 0.9114035440251502 Size of replay buffer: 10000 Learn step counter: 371078\n",
            "Total reward: 735.0\n",
            "Episode: 1341\n",
            "Total reward: 1053.0 Epsilon: 0.9113707340840418 Size of replay buffer: 10000 Learn step counter: 371222\n",
            "Total reward: 1053.0\n",
            "Episode: 1342\n",
            "Total reward: 1054.0 Epsilon: 0.9113404315071246 Size of replay buffer: 10000 Learn step counter: 371355\n",
            "Total reward: 1054.0\n",
            "Episode: 1343\n",
            "Total reward: 1003.0 Epsilon: 0.9112613761438767 Size of replay buffer: 10000 Learn step counter: 371702\n",
            "Total reward: 1003.0\n",
            "Episode: 1344\n",
            "Total reward: 239.0 Epsilon: 0.9112529470140781 Size of replay buffer: 10000 Learn step counter: 371739\n",
            "Total reward: 239.0\n",
            "Episode: 1345\n",
            "Total reward: 224.0 Epsilon: 0.9112426954748056 Size of replay buffer: 10000 Learn step counter: 371784\n",
            "Total reward: 224.0\n",
            "Episode: 1346\n",
            "Total reward: 598.0 Epsilon: 0.9112160420124364 Size of replay buffer: 10000 Learn step counter: 371901\n",
            "Total reward: 598.0\n",
            "Episode: 1347\n",
            "Total reward: 610.0 Epsilon: 0.9111944008857201 Size of replay buffer: 10000 Learn step counter: 371996\n",
            "Total reward: 610.0\n",
            "Episode: 1348\n",
            "Total reward: 234.0 Epsilon: 0.9111850611898081 Size of replay buffer: 10000 Learn step counter: 372037\n",
            "Total reward: 234.0\n",
            "Episode: 1349\n",
            "Total reward: 1032.0 Epsilon: 0.9111379085770751 Size of replay buffer: 10000 Learn step counter: 372244\n",
            "Total reward: 1032.0\n",
            "Episode: 1350\n",
            "Total reward: 1009.0 Epsilon: 0.9110668425829224 Size of replay buffer: 10000 Learn step counter: 372556\n",
            "Total reward: 1009.0\n",
            "Episode: 1351\n",
            "Total reward: 251.0 Epsilon: 0.9110595540764237 Size of replay buffer: 10000 Learn step counter: 372588\n",
            "Total reward: 251.0\n",
            "Episode: 1352\n",
            "Total reward: 606.0 Epsilon: 0.9110349557974635 Size of replay buffer: 10000 Learn step counter: 372696\n",
            "Total reward: 606.0\n",
            "Episode: 1353\n",
            "Total reward: 637.0 Epsilon: 0.9110001093724858 Size of replay buffer: 10000 Learn step counter: 372849\n",
            "Total reward: 637.0\n",
            "Episode: 1354\n",
            "Total reward: 618.0 Epsilon: 0.9109802953331059 Size of replay buffer: 10000 Learn step counter: 372936\n",
            "Total reward: 618.0\n",
            "Episode: 1355\n",
            "Total reward: 767.0 Epsilon: 0.9106243992758883 Size of replay buffer: 10000 Learn step counter: 374499\n",
            "Total reward: 767.0\n",
            "Episode: 1356\n",
            "Total reward: 1361.0 Epsilon: 0.9105012456389278 Size of replay buffer: 10000 Learn step counter: 375040\n",
            "Total reward: 1361.0\n",
            "Episode: 1357\n",
            "Total reward: 957.0 Epsilon: 0.9103735567776389 Size of replay buffer: 10000 Learn step counter: 375601\n",
            "Total reward: 957.0\n",
            "Episode: 1358\n",
            "Total reward: 712.0 Epsilon: 0.9103023228249717 Size of replay buffer: 10000 Learn step counter: 375914\n",
            "Total reward: 712.0\n",
            "Episode: 1359\n",
            "Total reward: 808.0 Epsilon: 0.9102613601369859 Size of replay buffer: 10000 Learn step counter: 376094\n",
            "Total reward: 808.0\n",
            "Episode: 1360\n",
            "Total reward: 786.0 Epsilon: 0.9101962787682782 Size of replay buffer: 10000 Learn step counter: 376380\n",
            "Total reward: 786.0\n",
            "Episode: 1361\n",
            "Total reward: 1202.0 Epsilon: 0.9100165327312343 Size of replay buffer: 10000 Learn step counter: 377170\n",
            "Total reward: 1202.0\n",
            "Episode: 1362\n",
            "Total reward: 1035.0 Epsilon: 0.909974445434605 Size of replay buffer: 10000 Learn step counter: 377355\n",
            "Total reward: 1035.0\n",
            "Episode: 1363\n",
            "Total reward: 635.0 Epsilon: 0.909937819695691 Size of replay buffer: 10000 Learn step counter: 377516\n",
            "Total reward: 635.0\n",
            "Episode: 1364\n",
            "Total reward: 760.0 Epsilon: 0.909842736149836 Size of replay buffer: 10000 Learn step counter: 377934\n",
            "Total reward: 760.0\n",
            "Episode: 1365\n",
            "Total reward: 796.0 Epsilon: 0.9097883746636136 Size of replay buffer: 10000 Learn step counter: 378173\n",
            "Total reward: 796.0\n",
            "Episode: 1366\n",
            "Total reward: 1124.0 Epsilon: 0.9095027459242856 Size of replay buffer: 10000 Learn step counter: 379429\n",
            "Total reward: 1124.0\n",
            "Episode: 1367\n",
            "Total reward: 615.0 Epsilon: 0.9094802360070694 Size of replay buffer: 10000 Learn step counter: 379528\n",
            "Total reward: 615.0\n",
            "Episode: 1368\n",
            "Total reward: 751.0 Epsilon: 0.9093756517805495 Size of replay buffer: 10000 Learn step counter: 379988\n",
            "Total reward: 751.0\n",
            "Episode: 1369\n",
            "Total reward: 1288.0 Epsilon: 0.9092751713101019 Size of replay buffer: 10000 Learn step counter: 380430\n",
            "Total reward: 1288.0\n",
            "Episode: 1370\n",
            "Total reward: 601.0 Epsilon: 0.9092508485215445 Size of replay buffer: 10000 Learn step counter: 380537\n",
            "Total reward: 601.0\n",
            "Episode: 1371\n",
            "Total reward: 1535.0 Epsilon: 0.9091837937357504 Size of replay buffer: 10000 Learn step counter: 380832\n",
            "Total reward: 1535.0\n",
            "Episode: 1372\n",
            "Total reward: 600.0 Epsilon: 0.9091615190028829 Size of replay buffer: 10000 Learn step counter: 380930\n",
            "Total reward: 600.0\n",
            "Episode: 1373\n",
            "Total reward: 228.0 Epsilon: 0.9091519728558561 Size of replay buffer: 10000 Learn step counter: 380972\n",
            "Total reward: 228.0\n",
            "Episode: 1374\n",
            "Total reward: 654.0 Epsilon: 0.9090615167235045 Size of replay buffer: 10000 Learn step counter: 381370\n",
            "Total reward: 654.0\n",
            "Episode: 1375\n",
            "Total reward: 244.0 Epsilon: 0.9090542442595507 Size of replay buffer: 10000 Learn step counter: 381402\n",
            "Total reward: 244.0\n",
            "Episode: 1376\n",
            "Total reward: 745.0 Epsilon: 0.9090117469616958 Size of replay buffer: 10000 Learn step counter: 381589\n",
            "Total reward: 745.0\n",
            "Episode: 1377\n",
            "Total reward: 611.0 Epsilon: 0.9089926579130566 Size of replay buffer: 10000 Learn step counter: 381673\n",
            "Total reward: 611.0\n",
            "Episode: 1378\n",
            "Total reward: 1028.0 Epsilon: 0.9089440281006318 Size of replay buffer: 10000 Learn step counter: 381887\n",
            "Total reward: 1028.0\n",
            "Episode: 1379\n",
            "Total reward: 230.0 Epsilon: 0.908936074874186 Size of replay buffer: 10000 Learn step counter: 381922\n",
            "Total reward: 230.0\n",
            "Episode: 1380\n",
            "Total reward: 638.0 Epsilon: 0.908902217631755 Size of replay buffer: 10000 Learn step counter: 382071\n",
            "Total reward: 638.0\n",
            "Episode: 1381\n",
            "Total reward: 578.0 Epsilon: 0.9088640445354782 Size of replay buffer: 10000 Learn step counter: 382239\n",
            "Total reward: 578.0\n",
            "Episode: 1382\n",
            "Total reward: 727.0 Epsilon: 0.9088354157653981 Size of replay buffer: 10000 Learn step counter: 382365\n",
            "Total reward: 727.0\n",
            "Episode: 1383\n",
            "Total reward: 251.0 Epsilon: 0.9088276906962289 Size of replay buffer: 10000 Learn step counter: 382399\n",
            "Total reward: 251.0\n",
            "Episode: 1384\n",
            "Total reward: 241.0 Epsilon: 0.9088199656927226 Size of replay buffer: 10000 Learn step counter: 382433\n",
            "Total reward: 241.0\n",
            "Episode: 1385\n",
            "Total reward: 611.0 Epsilon: 0.9088004262710647 Size of replay buffer: 10000 Learn step counter: 382519\n",
            "Total reward: 611.0\n",
            "Episode: 1386\n",
            "Total reward: 1297.0 Epsilon: 0.9087172748258916 Size of replay buffer: 10000 Learn step counter: 382885\n",
            "Total reward: 1297.0\n",
            "Episode: 1387\n",
            "Total reward: 731.0 Epsilon: 0.9085896090015374 Size of replay buffer: 10000 Learn step counter: 383447\n",
            "Total reward: 731.0\n",
            "Episode: 1388\n",
            "Total reward: 583.0 Epsilon: 0.9085589446158606 Size of replay buffer: 10000 Learn step counter: 383582\n",
            "Total reward: 583.0\n",
            "Episode: 1389\n",
            "Total reward: 608.0 Epsilon: 0.9085389565364473 Size of replay buffer: 10000 Learn step counter: 383670\n",
            "Total reward: 608.0\n",
            "Episode: 1390\n",
            "Total reward: 797.0 Epsilon: 0.9084855814339806 Size of replay buffer: 10000 Learn step counter: 383905\n",
            "Total reward: 797.0\n",
            "Episode: 1391\n",
            "Total reward: 1332.0 Epsilon: 0.9084460631657715 Size of replay buffer: 10000 Learn step counter: 384079\n",
            "Total reward: 1332.0\n",
            "Episode: 1392\n",
            "Total reward: 1184.0 Epsilon: 0.9082264728465709 Size of replay buffer: 10000 Learn step counter: 385046\n",
            "Total reward: 1184.0\n",
            "Episode: 1393\n",
            "Total reward: 589.0 Epsilon: 0.9081921879400624 Size of replay buffer: 10000 Learn step counter: 385197\n",
            "Total reward: 589.0\n",
            "Episode: 1394\n",
            "Total reward: 614.0 Epsilon: 0.9081737972321617 Size of replay buffer: 10000 Learn step counter: 385278\n",
            "Total reward: 614.0\n",
            "Episode: 1395\n",
            "Total reward: 231.0 Epsilon: 0.9081647155384613 Size of replay buffer: 10000 Learn step counter: 385318\n",
            "Total reward: 231.0\n",
            "Episode: 1396\n",
            "Total reward: 635.0 Epsilon: 0.9081277085756938 Size of replay buffer: 10000 Learn step counter: 385481\n",
            "Total reward: 635.0\n",
            "Episode: 1397\n",
            "Total reward: 637.0 Epsilon: 0.9080929733508093 Size of replay buffer: 10000 Learn step counter: 385634\n",
            "Total reward: 637.0\n",
            "Episode: 1398\n",
            "Total reward: 249.0 Epsilon: 0.9080852545923747 Size of replay buffer: 10000 Learn step counter: 385668\n",
            "Total reward: 249.0\n",
            "Episode: 1399\n",
            "Total reward: 232.0 Epsilon: 0.9080754927271364 Size of replay buffer: 10000 Learn step counter: 385711\n",
            "Total reward: 232.0\n",
            "Episode: 1400\n",
            "Total reward: 237.0 Epsilon: 0.9080668660498529 Size of replay buffer: 10000 Learn step counter: 385749\n",
            "Total reward: 237.0\n",
            "Episode: 1401\n",
            "Total reward: 226.0 Epsilon: 0.9080582394545222 Size of replay buffer: 10000 Learn step counter: 385787\n",
            "Total reward: 226.0\n",
            "Episode: 1402\n",
            "Total reward: 1030.0 Epsilon: 0.9080051195946377 Size of replay buffer: 10000 Learn step counter: 386021\n",
            "Total reward: 1030.0\n",
            "Episode: 1403\n",
            "Total reward: 648.0 Epsilon: 0.9078317071565096 Size of replay buffer: 10000 Learn step counter: 386785\n",
            "Total reward: 648.0\n",
            "Episode: 1404\n",
            "Total reward: 228.0 Epsilon: 0.9078224019280363 Size of replay buffer: 10000 Learn step counter: 386826\n",
            "Total reward: 228.0\n",
            "Episode: 1405\n",
            "Total reward: 239.0 Epsilon: 0.9078140046086053 Size of replay buffer: 10000 Learn step counter: 386863\n",
            "Total reward: 239.0\n",
            "Episode: 1406\n",
            "Total reward: 241.0 Epsilon: 0.9078069690764519 Size of replay buffer: 10000 Learn step counter: 386894\n",
            "Total reward: 241.0\n",
            "Episode: 1407\n",
            "Total reward: 232.0 Epsilon: 0.9077978910510153 Size of replay buffer: 10000 Learn step counter: 386934\n",
            "Total reward: 232.0\n",
            "Episode: 1408\n",
            "Total reward: 228.0 Epsilon: 0.9077872244871268 Size of replay buffer: 10000 Learn step counter: 386981\n",
            "Total reward: 228.0\n",
            "Episode: 1409\n",
            "Total reward: 243.0 Epsilon: 0.9077786005483788 Size of replay buffer: 10000 Learn step counter: 387019\n",
            "Total reward: 243.0\n",
            "Episode: 1410\n",
            "Total reward: 245.0 Epsilon: 0.9077704305767162 Size of replay buffer: 10000 Learn step counter: 387055\n",
            "Total reward: 245.0\n",
            "Episode: 1411\n",
            "Total reward: 805.0 Epsilon: 0.9077266317046155 Size of replay buffer: 10000 Learn step counter: 387248\n",
            "Total reward: 805.0\n",
            "Episode: 1412\n",
            "Total reward: 1016.0 Epsilon: 0.9076617315625373 Size of replay buffer: 10000 Learn step counter: 387534\n",
            "Total reward: 1016.0\n",
            "Episode: 1413\n",
            "Total reward: 1026.0 Epsilon: 0.9076099963118326 Size of replay buffer: 10000 Learn step counter: 387762\n",
            "Total reward: 1026.0\n",
            "Episode: 1414\n",
            "Total reward: 934.0 Epsilon: 0.9074525396175318 Size of replay buffer: 10000 Learn step counter: 388456\n",
            "Total reward: 934.0\n",
            "Episode: 1415\n",
            "Total reward: 619.0 Epsilon: 0.9074321221625326 Size of replay buffer: 10000 Learn step counter: 388546\n",
            "Total reward: 619.0\n",
            "Episode: 1416\n",
            "Total reward: 233.0 Epsilon: 0.9074239553091621 Size of replay buffer: 10000 Learn step counter: 388582\n",
            "Total reward: 233.0\n",
            "Episode: 1417\n",
            "Total reward: 675.0 Epsilon: 0.9072966980076321 Size of replay buffer: 10000 Learn step counter: 389143\n",
            "Total reward: 675.0\n",
            "Episode: 1418\n",
            "Total reward: 1029.0 Epsilon: 0.9072488393631064 Size of replay buffer: 10000 Learn step counter: 389354\n",
            "Total reward: 1029.0\n",
            "Episode: 1419\n",
            "Total reward: 1310.0 Epsilon: 0.907179891062755 Size of replay buffer: 10000 Learn step counter: 389658\n",
            "Total reward: 1310.0\n",
            "Episode: 1420\n",
            "Total reward: 626.0 Epsilon: 0.9071615208536612 Size of replay buffer: 10000 Learn step counter: 389739\n",
            "Total reward: 626.0\n",
            "Episode: 1421\n",
            "Total reward: 625.0 Epsilon: 0.9071404295908491 Size of replay buffer: 10000 Learn step counter: 389832\n",
            "Total reward: 625.0\n",
            "Episode: 1422\n",
            "Total reward: 729.0 Epsilon: 0.9070674077162991 Size of replay buffer: 10000 Learn step counter: 390154\n",
            "Total reward: 729.0\n",
            "Episode: 1423\n",
            "Total reward: 625.0 Epsilon: 0.9070454115956136 Size of replay buffer: 10000 Learn step counter: 390251\n",
            "Total reward: 625.0\n",
            "Episode: 1424\n",
            "Total reward: 248.0 Epsilon: 0.9070377017414172 Size of replay buffer: 10000 Learn step counter: 390285\n",
            "Total reward: 248.0\n",
            "Episode: 1425\n",
            "Total reward: 653.0 Epsilon: 0.906931811257965 Size of replay buffer: 10000 Learn step counter: 390752\n",
            "Total reward: 653.0\n",
            "Episode: 1426\n",
            "Total reward: 738.0 Epsilon: 0.9068964416026125 Size of replay buffer: 10000 Learn step counter: 390908\n",
            "Total reward: 738.0\n",
            "Episode: 1427\n",
            "Total reward: 650.0 Epsilon: 0.9068762633787453 Size of replay buffer: 10000 Learn step counter: 390997\n",
            "Total reward: 650.0\n",
            "Episode: 1428\n",
            "Total reward: 238.0 Epsilon: 0.9068674213771757 Size of replay buffer: 10000 Learn step counter: 391036\n",
            "Total reward: 238.0\n",
            "Episode: 1429\n",
            "Total reward: 240.0 Epsilon: 0.9068592596060899 Size of replay buffer: 10000 Learn step counter: 391072\n",
            "Total reward: 240.0\n",
            "Episode: 1430\n",
            "Total reward: 807.0 Epsilon: 0.9068175450343688 Size of replay buffer: 10000 Learn step counter: 391256\n",
            "Total reward: 807.0\n",
            "Episode: 1431\n",
            "Total reward: 233.0 Epsilon: 0.9068091570098225 Size of replay buffer: 10000 Learn step counter: 391293\n",
            "Total reward: 233.0\n",
            "Episode: 1432\n",
            "Total reward: 810.0 Epsilon: 0.9067710718202566 Size of replay buffer: 10000 Learn step counter: 391461\n",
            "Total reward: 810.0\n",
            "Episode: 1433\n",
            "Total reward: 1313.0 Epsilon: 0.9066890127412175 Size of replay buffer: 10000 Learn step counter: 391823\n",
            "Total reward: 1313.0\n",
            "Episode: 1434\n",
            "Total reward: 1023.0 Epsilon: 0.9066327997580136 Size of replay buffer: 10000 Learn step counter: 392071\n",
            "Total reward: 1023.0\n",
            "Episode: 1435\n",
            "Total reward: 598.0 Epsilon: 0.9066056011785981 Size of replay buffer: 10000 Learn step counter: 392191\n",
            "Total reward: 598.0\n",
            "Episode: 1436\n",
            "Total reward: 753.0 Epsilon: 0.9065666179710142 Size of replay buffer: 10000 Learn step counter: 392363\n",
            "Total reward: 753.0\n",
            "Episode: 1437\n",
            "Total reward: 994.0 Epsilon: 0.9064759658305558 Size of replay buffer: 10000 Learn step counter: 392763\n",
            "Total reward: 994.0\n",
            "Episode: 1438\n",
            "Total reward: 242.0 Epsilon: 0.9064675809656026 Size of replay buffer: 10000 Learn step counter: 392800\n",
            "Total reward: 242.0\n",
            "Episode: 1439\n",
            "Total reward: 752.0 Epsilon: 0.9063635697654282 Size of replay buffer: 10000 Learn step counter: 393259\n",
            "Total reward: 752.0\n",
            "Episode: 1440\n",
            "Total reward: 639.0 Epsilon: 0.9063309412601502 Size of replay buffer: 10000 Learn step counter: 393403\n",
            "Total reward: 639.0\n",
            "Episode: 1441\n",
            "Total reward: 630.0 Epsilon: 0.9063096427306239 Size of replay buffer: 10000 Learn step counter: 393497\n",
            "Total reward: 630.0\n",
            "Episode: 1442\n",
            "Total reward: 249.0 Epsilon: 0.9063026188572313 Size of replay buffer: 10000 Learn step counter: 393528\n",
            "Total reward: 249.0\n",
            "Episode: 1443\n",
            "Total reward: 812.0 Epsilon: 0.9062670471730883 Size of replay buffer: 10000 Learn step counter: 393685\n",
            "Total reward: 812.0\n",
            "Episode: 1444\n",
            "Total reward: 631.0 Epsilon: 0.906245297022237 Size of replay buffer: 10000 Learn step counter: 393781\n",
            "Total reward: 631.0\n",
            "Episode: 1445\n",
            "Total reward: 888.0 Epsilon: 0.9060230675873305 Size of replay buffer: 10000 Learn step counter: 394762\n",
            "Total reward: 888.0\n",
            "Episode: 1446\n",
            "Total reward: 247.0 Epsilon: 0.9060158194308754 Size of replay buffer: 10000 Learn step counter: 394794\n",
            "Total reward: 247.0\n",
            "Episode: 1447\n",
            "Total reward: 599.0 Epsilon: 0.9059915838288255 Size of replay buffer: 10000 Learn step counter: 394901\n",
            "Total reward: 599.0\n",
            "Episode: 1448\n",
            "Total reward: 1193.0 Epsilon: 0.9058106300606709 Size of replay buffer: 10000 Learn step counter: 395700\n",
            "Total reward: 1193.0\n",
            "Episode: 1449\n",
            "Total reward: 636.0 Epsilon: 0.9057746247992295 Size of replay buffer: 10000 Learn step counter: 395859\n",
            "Total reward: 636.0\n",
            "Episode: 1450\n",
            "Total reward: 888.0 Epsilon: 0.905564056652108 Size of replay buffer: 10000 Learn step counter: 396789\n",
            "Total reward: 888.0\n",
            "Episode: 1451\n",
            "Total reward: 670.0 Epsilon: 0.9054046913827494 Size of replay buffer: 10000 Learn step counter: 397493\n",
            "Total reward: 670.0\n",
            "Episode: 1452\n",
            "Total reward: 636.0 Epsilon: 0.905368928599284 Size of replay buffer: 10000 Learn step counter: 397651\n",
            "Total reward: 636.0\n",
            "Episode: 1453\n",
            "Total reward: 637.0 Epsilon: 0.9053505950618131 Size of replay buffer: 10000 Learn step counter: 397732\n",
            "Total reward: 637.0\n",
            "Episode: 1454\n",
            "Total reward: 239.0 Epsilon: 0.9053422206064924 Size of replay buffer: 10000 Learn step counter: 397769\n",
            "Total reward: 239.0\n",
            "Episode: 1455\n",
            "Total reward: 249.0 Epsilon: 0.905335430564451 Size of replay buffer: 10000 Learn step counter: 397799\n",
            "Total reward: 249.0\n",
            "Episode: 1456\n",
            "Total reward: 595.0 Epsilon: 0.9053116658183383 Size of replay buffer: 10000 Learn step counter: 397904\n",
            "Total reward: 595.0\n",
            "Episode: 1457\n",
            "Total reward: 947.0 Epsilon: 0.9051600387944573 Size of replay buffer: 10000 Learn step counter: 398574\n",
            "Total reward: 947.0\n",
            "Episode: 1458\n",
            "Total reward: 670.0 Epsilon: 0.904963414110554 Size of replay buffer: 10000 Learn step counter: 399443\n",
            "Total reward: 670.0\n",
            "Episode: 1459\n",
            "Total reward: 234.0 Epsilon: 0.9049539120434027 Size of replay buffer: 10000 Learn step counter: 399485\n",
            "Total reward: 234.0\n",
            "Episode: 1460\n",
            "Total reward: 606.0 Epsilon: 0.9049220129762359 Size of replay buffer: 10000 Learn step counter: 399626\n",
            "Total reward: 606.0\n",
            "Episode: 1461\n",
            "Total reward: 786.0 Epsilon: 0.9048568609286504 Size of replay buffer: 10000 Learn step counter: 399914\n",
            "Total reward: 786.0\n",
            "Episode: 1462\n",
            "Total reward: 228.0 Epsilon: 0.9048480386161611 Size of replay buffer: 10000 Learn step counter: 399953\n",
            "Total reward: 228.0\n",
            "Episode: 1463\n",
            "Total reward: 598.0 Epsilon: 0.9048258701080064 Size of replay buffer: 10000 Learn step counter: 400051\n",
            "Total reward: 598.0\n",
            "Episode: 1464\n",
            "Total reward: 1008.0 Epsilon: 0.9047530345480181 Size of replay buffer: 10000 Learn step counter: 400373\n",
            "Total reward: 1008.0\n",
            "Episode: 1465\n",
            "Total reward: 616.0 Epsilon: 0.9047335825644506 Size of replay buffer: 10000 Learn step counter: 400459\n",
            "Total reward: 616.0\n",
            "Episode: 1466\n",
            "Total reward: 245.0 Epsilon: 0.9047258923607199 Size of replay buffer: 10000 Learn step counter: 400493\n",
            "Total reward: 245.0\n",
            "Episode: 1467\n",
            "Total reward: 638.0 Epsilon: 0.904691965771638 Size of replay buffer: 10000 Learn step counter: 400643\n",
            "Total reward: 638.0\n",
            "Episode: 1468\n",
            "Total reward: 245.0 Epsilon: 0.9046847282639563 Size of replay buffer: 10000 Learn step counter: 400675\n",
            "Total reward: 245.0\n",
            "Episode: 1469\n",
            "Total reward: 1023.0 Epsilon: 0.9046243425661827 Size of replay buffer: 10000 Learn step counter: 400942\n",
            "Total reward: 1023.0\n",
            "Episode: 1470\n",
            "Total reward: 698.0 Epsilon: 0.9045368243838476 Size of replay buffer: 10000 Learn step counter: 401329\n",
            "Total reward: 698.0\n",
            "Episode: 1471\n",
            "Total reward: 608.0 Epsilon: 0.9045157941445265 Size of replay buffer: 10000 Learn step counter: 401422\n",
            "Total reward: 608.0\n",
            "Episode: 1472\n",
            "Total reward: 236.0 Epsilon: 0.9045078796649633 Size of replay buffer: 10000 Learn step counter: 401457\n",
            "Total reward: 236.0\n",
            "Episode: 1473\n",
            "Total reward: 630.0 Epsilon: 0.9044891113188338 Size of replay buffer: 10000 Learn step counter: 401540\n",
            "Total reward: 630.0\n",
            "Episode: 1474\n",
            "Total reward: 813.0 Epsilon: 0.9044545152676499 Size of replay buffer: 10000 Learn step counter: 401693\n",
            "Total reward: 813.0\n",
            "Episode: 1475\n",
            "Total reward: 647.0 Epsilon: 0.9042790680886592 Size of replay buffer: 10000 Learn step counter: 402469\n",
            "Total reward: 647.0\n",
            "Episode: 1476\n",
            "Total reward: 597.0 Epsilon: 0.9042578177775921 Size of replay buffer: 10000 Learn step counter: 402563\n",
            "Total reward: 597.0\n",
            "Episode: 1477\n",
            "Total reward: 957.0 Epsilon: 0.9041185727783715 Size of replay buffer: 10000 Learn step counter: 403179\n",
            "Total reward: 957.0\n",
            "Episode: 1478\n",
            "Total reward: 638.0 Epsilon: 0.9040851210058544 Size of replay buffer: 10000 Learn step counter: 403327\n",
            "Total reward: 638.0\n",
            "Episode: 1479\n",
            "Total reward: 603.0 Epsilon: 0.9040625191575252 Size of replay buffer: 10000 Learn step counter: 403427\n",
            "Total reward: 603.0\n",
            "Episode: 1480\n",
            "Total reward: 239.0 Epsilon: 0.9040546086441013 Size of replay buffer: 10000 Learn step counter: 403462\n",
            "Total reward: 239.0\n",
            "Episode: 1481\n",
            "Total reward: 1288.0 Epsilon: 0.9039524562322611 Size of replay buffer: 10000 Learn step counter: 403914\n",
            "Total reward: 1288.0\n",
            "Episode: 1482\n",
            "Total reward: 234.0 Epsilon: 0.9039445466818835 Size of replay buffer: 10000 Learn step counter: 403949\n",
            "Total reward: 234.0\n",
            "Episode: 1483\n",
            "Total reward: 680.0 Epsilon: 0.9037976676089157 Size of replay buffer: 10000 Learn step counter: 404599\n",
            "Total reward: 680.0\n",
            "Episode: 1484\n",
            "Total reward: 678.0 Epsilon: 0.903653297445322 Size of replay buffer: 10000 Learn step counter: 405238\n",
            "Total reward: 678.0\n",
            "Episode: 1485\n",
            "Total reward: 759.0 Epsilon: 0.9035911734088866 Size of replay buffer: 10000 Learn step counter: 405513\n",
            "Total reward: 759.0\n",
            "Episode: 1486\n",
            "Total reward: 242.0 Epsilon: 0.9035830411239036 Size of replay buffer: 10000 Learn step counter: 405549\n",
            "Total reward: 242.0\n",
            "Episode: 1487\n",
            "Total reward: 231.0 Epsilon: 0.9035742312310984 Size of replay buffer: 10000 Learn step counter: 405588\n",
            "Total reward: 231.0\n",
            "Episode: 1488\n",
            "Total reward: 638.0 Epsilon: 0.9035403478285048 Size of replay buffer: 10000 Learn step counter: 405738\n",
            "Total reward: 638.0\n",
            "Episode: 1489\n",
            "Total reward: 248.0 Epsilon: 0.9035326677672274 Size of replay buffer: 10000 Learn step counter: 405772\n",
            "Total reward: 248.0\n",
            "Episode: 1490\n",
            "Total reward: 952.0 Epsilon: 0.903402794264865 Size of replay buffer: 10000 Learn step counter: 406347\n",
            "Total reward: 952.0\n",
            "Episode: 1491\n",
            "Total reward: 617.0 Epsilon: 0.9033858556191531 Size of replay buffer: 10000 Learn step counter: 406422\n",
            "Total reward: 617.0\n",
            "Episode: 1492\n",
            "Total reward: 1895.0 Epsilon: 0.9033165234067158 Size of replay buffer: 10000 Learn step counter: 406729\n",
            "Total reward: 1895.0\n",
            "Episode: 1493\n",
            "Total reward: 792.0 Epsilon: 0.9032580355480707 Size of replay buffer: 10000 Learn step counter: 406988\n",
            "Total reward: 792.0\n",
            "Episode: 1494\n",
            "Total reward: 742.0 Epsilon: 0.9032189704779326 Size of replay buffer: 10000 Learn step counter: 407161\n",
            "Total reward: 742.0\n",
            "Episode: 1495\n",
            "Total reward: 1032.0 Epsilon: 0.9031722300997791 Size of replay buffer: 10000 Learn step counter: 407368\n",
            "Total reward: 1032.0\n",
            "Episode: 1496\n",
            "Total reward: 782.0 Epsilon: 0.9031403938358017 Size of replay buffer: 10000 Learn step counter: 407509\n",
            "Total reward: 782.0\n",
            "Episode: 1497\n",
            "Total reward: 594.0 Epsilon: 0.9031166867086543 Size of replay buffer: 10000 Learn step counter: 407614\n",
            "Total reward: 594.0\n",
            "Episode: 1498\n",
            "Total reward: 246.0 Epsilon: 0.9031081071398099 Size of replay buffer: 10000 Learn step counter: 407652\n",
            "Total reward: 246.0\n",
            "Episode: 1499\n",
            "Total reward: 916.0 Epsilon: 0.9029313410072672 Size of replay buffer: 10000 Learn step counter: 408435\n",
            "Total reward: 916.0\n",
            "Episode: 1500\n",
            "Total reward: 230.0 Epsilon: 0.9029223117378736 Size of replay buffer: 10000 Learn step counter: 408475\n",
            "Total reward: 230.0\n",
            "Episode: 1501\n",
            "Total reward: 1286.0 Epsilon: 0.9028110334066946 Size of replay buffer: 10000 Learn step counter: 408968\n",
            "Total reward: 1286.0\n",
            "Episode: 1502\n",
            "Total reward: 637.0 Epsilon: 0.9027918488736703 Size of replay buffer: 10000 Learn step counter: 409053\n",
            "Total reward: 637.0\n",
            "Episode: 1503\n",
            "Total reward: 233.0 Epsilon: 0.9027830466949529 Size of replay buffer: 10000 Learn step counter: 409092\n",
            "Total reward: 233.0\n",
            "Episode: 1504\n",
            "Total reward: 639.0 Epsilon: 0.9027512241494503 Size of replay buffer: 10000 Learn step counter: 409233\n",
            "Total reward: 639.0\n",
            "Episode: 1505\n",
            "Total reward: 842.0 Epsilon: 0.9024876592413655 Size of replay buffer: 10000 Learn step counter: 410401\n",
            "Total reward: 842.0\n",
            "Episode: 1506\n",
            "Total reward: 248.0 Epsilon: 0.9024806649882338 Size of replay buffer: 10000 Learn step counter: 410432\n",
            "Total reward: 248.0\n",
            "Episode: 1507\n",
            "Total reward: 239.0 Epsilon: 0.9024725426977829 Size of replay buffer: 10000 Learn step counter: 410468\n",
            "Total reward: 239.0\n",
            "Episode: 1508\n",
            "Total reward: 618.0 Epsilon: 0.9024502067759641 Size of replay buffer: 10000 Learn step counter: 410567\n",
            "Total reward: 618.0\n",
            "Episode: 1509\n",
            "Total reward: 1068.0 Epsilon: 0.9023209400266213 Size of replay buffer: 10000 Learn step counter: 411140\n",
            "Total reward: 1068.0\n",
            "Episode: 1510\n",
            "Total reward: 939.0 Epsilon: 0.9021763546631006 Size of replay buffer: 10000 Learn step counter: 411781\n",
            "Total reward: 939.0\n",
            "Episode: 1511\n",
            "Total reward: 638.0 Epsilon: 0.9021434258229895 Size of replay buffer: 10000 Learn step counter: 411927\n",
            "Total reward: 638.0\n",
            "Episode: 1512\n",
            "Total reward: 1600.0 Epsilon: 0.9019921039368493 Size of replay buffer: 10000 Learn step counter: 412598\n",
            "Total reward: 1600.0\n",
            "Episode: 1513\n",
            "Total reward: 230.0 Epsilon: 0.9019830840597806 Size of replay buffer: 10000 Learn step counter: 412638\n",
            "Total reward: 230.0\n",
            "Episode: 1514\n",
            "Total reward: 610.0 Epsilon: 0.9019517406882813 Size of replay buffer: 10000 Learn step counter: 412777\n",
            "Total reward: 610.0\n",
            "Episode: 1515\n",
            "Total reward: 580.0 Epsilon: 0.9019237806142045 Size of replay buffer: 10000 Learn step counter: 412901\n",
            "Total reward: 580.0\n",
            "Episode: 1516\n",
            "Total reward: 610.0 Epsilon: 0.9018737252271662 Size of replay buffer: 10000 Learn step counter: 413123\n",
            "Total reward: 610.0\n",
            "Episode: 1517\n",
            "Total reward: 1079.0 Epsilon: 0.9017492752247593 Size of replay buffer: 10000 Learn step counter: 413675\n",
            "Total reward: 1079.0\n",
            "Episode: 1518\n",
            "Total reward: 755.0 Epsilon: 0.9016496374224685 Size of replay buffer: 10000 Learn step counter: 414117\n",
            "Total reward: 755.0\n",
            "Episode: 1519\n",
            "Total reward: 601.0 Epsilon: 0.9016270964604752 Size of replay buffer: 10000 Learn step counter: 414217\n",
            "Total reward: 601.0\n",
            "Episode: 1520\n",
            "Total reward: 1008.0 Epsilon: 0.9015531660605014 Size of replay buffer: 10000 Learn step counter: 414545\n",
            "Total reward: 1008.0\n",
            "Episode: 1521\n",
            "Total reward: 1315.0 Epsilon: 0.9014956938709187 Size of replay buffer: 10000 Learn step counter: 414800\n",
            "Total reward: 1315.0\n",
            "Episode: 1522\n",
            "Total reward: 739.0 Epsilon: 0.90137738031079 Size of replay buffer: 10000 Learn step counter: 415325\n",
            "Total reward: 739.0\n",
            "Episode: 1523\n",
            "Total reward: 862.0 Epsilon: 0.9011389974913836 Size of replay buffer: 10000 Learn step counter: 416383\n",
            "Total reward: 862.0\n",
            "Episode: 1524\n",
            "Total reward: 707.0 Epsilon: 0.9010838044111855 Size of replay buffer: 10000 Learn step counter: 416628\n",
            "Total reward: 707.0\n",
            "Episode: 1525\n",
            "Total reward: 741.0 Epsilon: 0.9010225327880764 Size of replay buffer: 10000 Learn step counter: 416900\n",
            "Total reward: 741.0\n",
            "Episode: 1526\n",
            "Total reward: 614.0 Epsilon: 0.9010018095055478 Size of replay buffer: 10000 Learn step counter: 416992\n",
            "Total reward: 614.0\n",
            "Episode: 1527\n",
            "Total reward: 1378.0 Epsilon: 0.9008979750175861 Size of replay buffer: 10000 Learn step counter: 417453\n",
            "Total reward: 1378.0\n",
            "Episode: 1528\n",
            "Total reward: 251.0 Epsilon: 0.9008916687530437 Size of replay buffer: 10000 Learn step counter: 417481\n",
            "Total reward: 251.0\n",
            "Episode: 1529\n",
            "Total reward: 634.0 Epsilon: 0.9008732006608219 Size of replay buffer: 10000 Learn step counter: 417563\n",
            "Total reward: 634.0\n",
            "Episode: 1530\n",
            "Total reward: 736.0 Epsilon: 0.9007518161603072 Size of replay buffer: 10000 Learn step counter: 418102\n",
            "Total reward: 736.0\n",
            "Episode: 1531\n",
            "Total reward: 1532.0 Epsilon: 0.9006666991248716 Size of replay buffer: 10000 Learn step counter: 418480\n",
            "Total reward: 1532.0\n",
            "Episode: 1532\n",
            "Total reward: 232.0 Epsilon: 0.9006576925017863 Size of replay buffer: 10000 Learn step counter: 418520\n",
            "Total reward: 232.0\n",
            "Episode: 1533\n",
            "Total reward: 233.0 Epsilon: 0.9006491362932789 Size of replay buffer: 10000 Learn step counter: 418558\n",
            "Total reward: 233.0\n",
            "Episode: 1534\n",
            "Total reward: 637.0 Epsilon: 0.9006149122720783 Size of replay buffer: 10000 Learn step counter: 418710\n",
            "Total reward: 637.0\n",
            "Episode: 1535\n",
            "Total reward: 766.0 Epsilon: 0.900527556851463 Size of replay buffer: 10000 Learn step counter: 419098\n",
            "Total reward: 766.0\n",
            "Episode: 1536\n",
            "Total reward: 803.0 Epsilon: 0.9004820813524173 Size of replay buffer: 10000 Learn step counter: 419300\n",
            "Total reward: 803.0\n",
            "Episode: 1537\n",
            "Total reward: 229.0 Epsilon: 0.9004733016938263 Size of replay buffer: 10000 Learn step counter: 419339\n",
            "Total reward: 229.0\n",
            "Episode: 1538\n",
            "Total reward: 1010.0 Epsilon: 0.9003954141121692 Size of replay buffer: 10000 Learn step counter: 419685\n",
            "Total reward: 1010.0\n",
            "Episode: 1539\n",
            "Total reward: 623.0 Epsilon: 0.900373804878838 Size of replay buffer: 10000 Learn step counter: 419781\n",
            "Total reward: 623.0\n",
            "Episode: 1540\n",
            "Total reward: 228.0 Epsilon: 0.9003645760934808 Size of replay buffer: 10000 Learn step counter: 419822\n",
            "Total reward: 228.0\n",
            "Episode: 1541\n",
            "Total reward: 639.0 Epsilon: 0.9003326137143649 Size of replay buffer: 10000 Learn step counter: 419964\n",
            "Total reward: 639.0\n",
            "Episode: 1542\n",
            "Total reward: 596.0 Epsilon: 0.9003116812218179 Size of replay buffer: 10000 Learn step counter: 420057\n",
            "Total reward: 596.0\n",
            "Episode: 1543\n",
            "Total reward: 629.0 Epsilon: 0.9002898489255339 Size of replay buffer: 10000 Learn step counter: 420154\n",
            "Total reward: 629.0\n",
            "Episode: 1544\n",
            "Total reward: 790.0 Epsilon: 0.9002290814040543 Size of replay buffer: 10000 Learn step counter: 420424\n",
            "Total reward: 790.0\n",
            "Episode: 1545\n",
            "Total reward: 213.0 Epsilon: 0.9002185037731679 Size of replay buffer: 10000 Learn step counter: 420471\n",
            "Total reward: 213.0\n",
            "Episode: 1546\n",
            "Total reward: 612.0 Epsilon: 0.9001948733446375 Size of replay buffer: 10000 Learn step counter: 420576\n",
            "Total reward: 612.0\n",
            "Episode: 1547\n",
            "Total reward: 981.0 Epsilon: 0.9000661546652458 Size of replay buffer: 10000 Learn step counter: 421148\n",
            "Total reward: 981.0\n",
            "Episode: 1548\n",
            "Total reward: 1693.0 Epsilon: 0.8997703065125823 Size of replay buffer: 10000 Learn step counter: 422463\n",
            "Total reward: 1693.0\n",
            "Episode: 1549\n",
            "Total reward: 899.0 Epsilon: 0.8995762019844415 Size of replay buffer: 10000 Learn step counter: 423326\n",
            "Total reward: 899.0\n",
            "Episode: 1550\n",
            "Total reward: 1238.0 Epsilon: 0.8994217130195384 Size of replay buffer: 10000 Learn step counter: 424013\n",
            "Total reward: 1238.0\n",
            "Episode: 1551\n",
            "Total reward: 244.0 Epsilon: 0.8994133934061301 Size of replay buffer: 10000 Learn step counter: 424050\n",
            "Total reward: 244.0\n",
            "Episode: 1552\n",
            "Total reward: 626.0 Epsilon: 0.8993956301648001 Size of replay buffer: 10000 Learn step counter: 424129\n",
            "Total reward: 626.0\n",
            "Episode: 1553\n",
            "Total reward: 604.0 Epsilon: 0.899373145552291 Size of replay buffer: 10000 Learn step counter: 424229\n",
            "Total reward: 604.0\n",
            "Episode: 1554\n",
            "Total reward: 610.0 Epsilon: 0.8993499869890628 Size of replay buffer: 10000 Learn step counter: 424332\n",
            "Total reward: 610.0\n",
            "Episode: 1555\n",
            "Total reward: 624.0 Epsilon: 0.8993016482203289 Size of replay buffer: 10000 Learn step counter: 424547\n",
            "Total reward: 624.0\n",
            "Episode: 1556\n",
            "Total reward: 627.0 Epsilon: 0.8992793907771873 Size of replay buffer: 10000 Learn step counter: 424646\n",
            "Total reward: 627.0\n",
            "Episode: 1557\n",
            "Total reward: 240.0 Epsilon: 0.899271522115959 Size of replay buffer: 10000 Learn step counter: 424681\n",
            "Total reward: 240.0\n",
            "Episode: 1558\n",
            "Total reward: 250.0 Epsilon: 0.8992641031555763 Size of replay buffer: 10000 Learn step counter: 424714\n",
            "Total reward: 250.0\n",
            "Episode: 1559\n",
            "Total reward: 772.0 Epsilon: 0.899183397813925 Size of replay buffer: 10000 Learn step counter: 425073\n",
            "Total reward: 772.0\n",
            "Episode: 1560\n",
            "Total reward: 731.0 Epsilon: 0.8990563971127262 Size of replay buffer: 10000 Learn step counter: 425638\n",
            "Total reward: 731.0\n",
            "Episode: 1561\n",
            "Total reward: 1028.0 Epsilon: 0.8990091978849619 Size of replay buffer: 10000 Learn step counter: 425848\n",
            "Total reward: 1028.0\n",
            "Episode: 1562\n",
            "Total reward: 989.0 Epsilon: 0.8985997923932746 Size of replay buffer: 10000 Learn step counter: 427670\n",
            "Total reward: 989.0\n",
            "Episode: 1563\n",
            "Total reward: 621.0 Epsilon: 0.8985800234128277 Size of replay buffer: 10000 Learn step counter: 427758\n",
            "Total reward: 621.0\n",
            "Episode: 1564\n",
            "Total reward: 637.0 Epsilon: 0.8985461026529575 Size of replay buffer: 10000 Learn step counter: 427909\n",
            "Total reward: 637.0\n",
            "Episode: 1565\n",
            "Total reward: 639.0 Epsilon: 0.8985144294571189 Size of replay buffer: 10000 Learn step counter: 428050\n",
            "Total reward: 639.0\n",
            "Episode: 1566\n",
            "Total reward: 243.0 Epsilon: 0.8985070167427258 Size of replay buffer: 10000 Learn step counter: 428083\n",
            "Total reward: 243.0\n",
            "Episode: 1567\n",
            "Total reward: 237.0 Epsilon: 0.8984984809655436 Size of replay buffer: 10000 Learn step counter: 428121\n",
            "Total reward: 237.0\n",
            "Episode: 1568\n",
            "Total reward: 234.0 Epsilon: 0.8984906191372468 Size of replay buffer: 10000 Learn step counter: 428156\n",
            "Total reward: 234.0\n",
            "Episode: 1569\n",
            "Total reward: 240.0 Epsilon: 0.8984820835158411 Size of replay buffer: 10000 Learn step counter: 428194\n",
            "Total reward: 240.0\n",
            "Episode: 1570\n",
            "Total reward: 657.0 Epsilon: 0.8983938119764996 Size of replay buffer: 10000 Learn step counter: 428587\n",
            "Total reward: 657.0\n",
            "Episode: 1571\n",
            "Total reward: 235.0 Epsilon: 0.8983861756605969 Size of replay buffer: 10000 Learn step counter: 428621\n",
            "Total reward: 235.0\n",
            "Episode: 1572\n",
            "Total reward: 1181.0 Epsilon: 0.8981649752735759 Size of replay buffer: 10000 Learn step counter: 429606\n",
            "Total reward: 1181.0\n",
            "Episode: 1573\n",
            "Total reward: 242.0 Epsilon: 0.8981566672849394 Size of replay buffer: 10000 Learn step counter: 429643\n",
            "Total reward: 242.0\n",
            "Episode: 1574\n",
            "Total reward: 624.0 Epsilon: 0.8981074945473679 Size of replay buffer: 10000 Learn step counter: 429862\n",
            "Total reward: 624.0\n",
            "Episode: 1575\n",
            "Total reward: 229.0 Epsilon: 0.8980982889915754 Size of replay buffer: 10000 Learn step counter: 429903\n",
            "Total reward: 229.0\n",
            "Episode: 1576\n",
            "Total reward: 1001.0 Epsilon: 0.8980172392682751 Size of replay buffer: 10000 Learn step counter: 430264\n",
            "Total reward: 1001.0\n",
            "Episode: 1577\n",
            "Total reward: 748.0 Epsilon: 0.8979094836515453 Size of replay buffer: 10000 Learn step counter: 430744\n",
            "Total reward: 748.0\n",
            "Episode: 1578\n",
            "Total reward: 639.0 Epsilon: 0.8978771594879273 Size of replay buffer: 10000 Learn step counter: 430888\n",
            "Total reward: 639.0\n",
            "Episode: 1579\n",
            "Total reward: 1008.0 Epsilon: 0.8978026387670173 Size of replay buffer: 10000 Learn step counter: 431220\n",
            "Total reward: 1008.0\n",
            "Episode: 1580\n",
            "Total reward: 970.0 Epsilon: 0.8976785261161605 Size of replay buffer: 10000 Learn step counter: 431773\n",
            "Total reward: 970.0\n",
            "Episode: 1581\n",
            "Total reward: 604.0 Epsilon: 0.8976572065016688 Size of replay buffer: 10000 Learn step counter: 431868\n",
            "Total reward: 604.0\n",
            "Episode: 1582\n",
            "Total reward: 248.0 Epsilon: 0.8976504740970239 Size of replay buffer: 10000 Learn step counter: 431898\n",
            "Total reward: 248.0\n",
            "Episode: 1583\n",
            "Total reward: 993.0 Epsilon: 0.8975598159661081 Size of replay buffer: 10000 Learn step counter: 432302\n",
            "Total reward: 993.0\n",
            "Episode: 1584\n",
            "Total reward: 639.0 Epsilon: 0.8975275043903018 Size of replay buffer: 10000 Learn step counter: 432446\n",
            "Total reward: 639.0\n",
            "Episode: 1585\n",
            "Total reward: 1391.0 Epsilon: 0.8974626603625131 Size of replay buffer: 10000 Learn step counter: 432735\n",
            "Total reward: 1391.0\n",
            "Episode: 1586\n",
            "Total reward: 730.0 Epsilon: 0.8974319227889331 Size of replay buffer: 10000 Learn step counter: 432872\n",
            "Total reward: 730.0\n",
            "Episode: 1587\n",
            "Total reward: 250.0 Epsilon: 0.8974247433613701 Size of replay buffer: 10000 Learn step counter: 432904\n",
            "Total reward: 250.0\n",
            "Episode: 1588\n",
            "Total reward: 597.0 Epsilon: 0.897399391467295 Size of replay buffer: 10000 Learn step counter: 433017\n",
            "Total reward: 597.0\n",
            "Episode: 1589\n",
            "Total reward: 640.0 Epsilon: 0.8973691047451342 Size of replay buffer: 10000 Learn step counter: 433152\n",
            "Total reward: 640.0\n",
            "Episode: 1590\n",
            "Total reward: 1262.0 Epsilon: 0.8972419116736419 Size of replay buffer: 10000 Learn step counter: 433719\n",
            "Total reward: 1262.0\n",
            "Episode: 1591\n",
            "Total reward: 993.0 Epsilon: 0.897153313398389 Size of replay buffer: 10000 Learn step counter: 434114\n",
            "Total reward: 993.0\n",
            "Episode: 1592\n",
            "Total reward: 995.0 Epsilon: 0.8970667422699365 Size of replay buffer: 10000 Learn step counter: 434500\n",
            "Total reward: 995.0\n",
            "Episode: 1593\n",
            "Total reward: 234.0 Epsilon: 0.8970582201753 Size of replay buffer: 10000 Learn step counter: 434538\n",
            "Total reward: 234.0\n",
            "Episode: 1594\n",
            "Total reward: 239.0 Epsilon: 0.8970496981616232 Size of replay buffer: 10000 Learn step counter: 434576\n",
            "Total reward: 239.0\n",
            "Episode: 1595\n",
            "Total reward: 1268.0 Epsilon: 0.8969214292101821 Size of replay buffer: 10000 Learn step counter: 435148\n",
            "Total reward: 1268.0\n",
            "Episode: 1596\n",
            "Total reward: 242.0 Epsilon: 0.8969133569526343 Size of replay buffer: 10000 Learn step counter: 435184\n",
            "Total reward: 242.0\n",
            "Episode: 1597\n",
            "Total reward: 226.0 Epsilon: 0.8969037151846649 Size of replay buffer: 10000 Learn step counter: 435227\n",
            "Total reward: 226.0\n",
            "Episode: 1598\n",
            "Total reward: 1320.0 Epsilon: 0.8968602164038892 Size of replay buffer: 10000 Learn step counter: 435421\n",
            "Total reward: 1320.0\n",
            "Episode: 1599\n",
            "Total reward: 601.0 Epsilon: 0.8968330867892854 Size of replay buffer: 10000 Learn step counter: 435542\n",
            "Total reward: 601.0\n",
            "Episode: 1600\n",
            "Total reward: 1254.0 Epsilon: 0.8967052971636533 Size of replay buffer: 10000 Learn step counter: 436112\n",
            "Total reward: 1254.0\n",
            "Episode: 1601\n",
            "Total reward: 622.0 Epsilon: 0.8966882599227295 Size of replay buffer: 10000 Learn step counter: 436188\n",
            "Total reward: 622.0\n",
            "Episode: 1602\n",
            "Total reward: 242.0 Epsilon: 0.8966799655936486 Size of replay buffer: 10000 Learn step counter: 436225\n",
            "Total reward: 242.0\n",
            "Episode: 1603\n",
            "Total reward: 1025.0 Epsilon: 0.8966263905595703 Size of replay buffer: 10000 Learn step counter: 436464\n",
            "Total reward: 1025.0\n",
            "Episode: 1604\n",
            "Total reward: 737.0 Epsilon: 0.8965454736691258 Size of replay buffer: 10000 Learn step counter: 436825\n",
            "Total reward: 737.0\n",
            "Episode: 1605\n",
            "Total reward: 780.0 Epsilon: 0.8964746493654401 Size of replay buffer: 10000 Learn step counter: 437141\n",
            "Total reward: 780.0\n",
            "Episode: 1606\n",
            "Total reward: 785.0 Epsilon: 0.8964085367897074 Size of replay buffer: 10000 Learn step counter: 437436\n",
            "Total reward: 785.0\n",
            "Episode: 1607\n",
            "Total reward: 774.0 Epsilon: 0.896370440231684 Size of replay buffer: 10000 Learn step counter: 437606\n",
            "Total reward: 774.0\n",
            "Episode: 1608\n",
            "Total reward: 232.0 Epsilon: 0.8963608043000388 Size of replay buffer: 10000 Learn step counter: 437649\n",
            "Total reward: 232.0\n",
            "Episode: 1609\n",
            "Total reward: 633.0 Epsilon: 0.8963435495184748 Size of replay buffer: 10000 Learn step counter: 437726\n",
            "Total reward: 633.0\n",
            "Episode: 1610\n",
            "Total reward: 657.0 Epsilon: 0.896219862627755 Size of replay buffer: 10000 Learn step counter: 438278\n",
            "Total reward: 657.0\n",
            "Episode: 1611\n",
            "Total reward: 750.0 Epsilon: 0.8961302451112269 Size of replay buffer: 10000 Learn step counter: 438678\n",
            "Total reward: 750.0\n",
            "Episode: 1612\n",
            "Total reward: 637.0 Epsilon: 0.8960964168287536 Size of replay buffer: 10000 Learn step counter: 438829\n",
            "Total reward: 637.0\n",
            "Episode: 1613\n",
            "Total reward: 634.0 Epsilon: 0.8960780470382008 Size of replay buffer: 10000 Learn step counter: 438911\n",
            "Total reward: 634.0\n",
            "Episode: 1614\n",
            "Total reward: 691.0 Epsilon: 0.8960222679089359 Size of replay buffer: 10000 Learn step counter: 439160\n",
            "Total reward: 691.0\n",
            "Episode: 1615\n",
            "Total reward: 231.0 Epsilon: 0.8960135317333194 Size of replay buffer: 10000 Learn step counter: 439199\n",
            "Total reward: 231.0\n",
            "Episode: 1616\n",
            "Total reward: 247.0 Epsilon: 0.8960068116561909 Size of replay buffer: 10000 Learn step counter: 439229\n",
            "Total reward: 247.0\n",
            "Episode: 1617\n",
            "Total reward: 792.0 Epsilon: 0.8959496930354754 Size of replay buffer: 10000 Learn step counter: 439484\n",
            "Total reward: 792.0\n",
            "Episode: 1618\n",
            "Total reward: 236.0 Epsilon: 0.8959418535089783 Size of replay buffer: 10000 Learn step counter: 439519\n",
            "Total reward: 236.0\n",
            "Episode: 1619\n",
            "Total reward: 631.0 Epsilon: 0.8959207991201762 Size of replay buffer: 10000 Learn step counter: 439613\n",
            "Total reward: 631.0\n",
            "Episode: 1620\n",
            "Total reward: 240.0 Epsilon: 0.8959111680221477 Size of replay buffer: 10000 Learn step counter: 439656\n",
            "Total reward: 240.0\n",
            "Episode: 1621\n",
            "Total reward: 246.0 Epsilon: 0.8959037767845754 Size of replay buffer: 10000 Learn step counter: 439689\n",
            "Total reward: 246.0\n",
            "Episode: 1622\n",
            "Total reward: 637.0 Epsilon: 0.895885858885977 Size of replay buffer: 10000 Learn step counter: 439769\n",
            "Total reward: 637.0\n",
            "Episode: 1623\n",
            "Total reward: 233.0 Epsilon: 0.8958782438875874 Size of replay buffer: 10000 Learn step counter: 439803\n",
            "Total reward: 233.0\n",
            "Episode: 1624\n",
            "Total reward: 248.0 Epsilon: 0.8958710768894073 Size of replay buffer: 10000 Learn step counter: 439835\n",
            "Total reward: 248.0\n",
            "Episode: 1625\n",
            "Total reward: 651.0 Epsilon: 0.8958527117182765 Size of replay buffer: 10000 Learn step counter: 439917\n",
            "Total reward: 651.0\n",
            "Episode: 1626\n",
            "Total reward: 811.0 Epsilon: 0.8958168783219963 Size of replay buffer: 10000 Learn step counter: 440077\n",
            "Total reward: 811.0\n",
            "Episode: 1627\n",
            "Total reward: 241.0 Epsilon: 0.8958083681010109 Size of replay buffer: 10000 Learn step counter: 440115\n",
            "Total reward: 241.0\n",
            "Episode: 1628\n",
            "Total reward: 773.0 Epsilon: 0.895774551969167 Size of replay buffer: 10000 Learn step counter: 440266\n",
            "Total reward: 773.0\n",
            "Episode: 1629\n"
          ]
        }
      ],
      "source": [
        "model_path = os.path.join(\"models\", get_current_date_time_string())\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"CUDA is not available\")\n",
        "\n",
        "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
        "SHOULD_TRAIN = False\n",
        "DISPLAY = True\n",
        "CKPT_SAVE_INTERVAL = 500 #after how much episodes should the model be saved\n",
        "NUM_OF_EPISODES = 5000 #Total episodes\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym_super_mario_bros.make(ENV_NAME, render_mode='human' if DISPLAY else 'rgb', apply_api_compatibility=True)\n",
        "env = JoypadSpace(env, RIGHT_ONLY)\n",
        "env = apply_wrappers(env)\n",
        "\n",
        "# Create the agent\n",
        "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
        "\n",
        "if not SHOULD_TRAIN:\n",
        "    folder_name = \"\"\n",
        "    ckpt_name = \"\"\n",
        "    # agent.load_model(os.path.join(\"models\", folder_name, ckpt_name))\n",
        "    agent.load_model('model_1500_iter.pt')\n",
        "    agent.epsilon = 0.2\n",
        "    agent.eps_min = 0.0\n",
        "    agent.eps_decay = 0.0\n",
        "\n",
        "# Training loop\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "rewards = []\n",
        "episodes = []\n",
        "for i in range(NUM_OF_EPISODES):    \n",
        "    print(\"Episode:\", i)\n",
        "    done = False\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)                               #Choosing action\n",
        "        new_state, reward, done, truncated, info  = env.step(action)      #Taking a step based on the policy's action\n",
        "        total_reward += reward\n",
        "\n",
        "        if SHOULD_TRAIN:\n",
        "            agent.store_in_memory(state, action, reward, new_state, done)   #Store in replay buffer\n",
        "            agent.learn()\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "    print(\"Total reward:\", total_reward, \"Epsilon:\", agent.epsilon, \"Size of replay buffer:\", len(agent.replay_buffer), \"Learn step counter:\", agent.learn_step_counter)\n",
        "\n",
        "    rewards.append(total_reward)\n",
        "    episodes.append(i)\n",
        "    if SHOULD_TRAIN and (i + 1) % CKPT_SAVE_INTERVAL == 0:\n",
        "        agent.save_model(os.path.join(model_path, \"model_\" + str(i + 1) + \"_iter.pt\"))\n",
        "\n",
        "    print(\"Total reward:\", total_reward)\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Plot rewards\n",
        "plt.plot(episodes, rewards, label='Array 1')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('Average rewards over the episodes')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the plot as an image\n",
        "plt.savefig('1500.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HGRFqThY3hM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
